{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n",
      "90\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# feel a little weird, but it's much better than ugly 'dir > a text'\n",
    "\n",
    "# read images folders to create image files lists\n",
    "\n",
    "import codecs\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "n=3 # n folders and outfiles\n",
    "\n",
    "# add u will read unicode name like piñata.gif correctly\n",
    "folder = [u\"E:\\\\dictIllustration\\\\\",\n",
    "    u\"E:\\\\dictIllustration\\\\mwl-grpbw\\\\\",\n",
    "    u\"E:\\\\dictIllustration\\\\mwl-grpcolor\\\\\"]\n",
    "\n",
    "outfile = [\"E:\\\\1Now\\\\taglist\\\\mwlimgSingle.txt\", # black-and-white single images\n",
    "     \"E:\\\\1Now\\\\taglist\\\\mwlimgGrpbw.txt\", # black-and-white group images\n",
    "     \"E:\\\\1Now\\\\taglist\\\\mwlimgGrpcol.txt\"] # color pages\n",
    "\n",
    "cntx = [0, 0, 0] # how many files in each folder\n",
    "\n",
    "for i in range(0,n):\n",
    "    with codecs.open(outfile[i], 'w', 'utf-8') as f:\n",
    "        for finfolder in os.listdir(folder[i]):\n",
    "            if isfile(join(folder[i], finfolder)):\n",
    "                f.write(finfolder+'\\n')\n",
    "                cntx[i] += 1\n",
    "        f.close()\n",
    "for k in cntx:\n",
    "    print k\n",
    "\n",
    "# for i in os.listdir(x):\n",
    "#     if isfile(join(x, i)):\n",
    "#         print i, '...', join(x, i)\n",
    "#         cnt1 += 1\n",
    "#     else:\n",
    "#         print 'not a file: ', i\n",
    "#         cnt2 += 1\n",
    "# print 'number of files: ', cnt1, 'number of non-files: ', cnt2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# create empty group images list files\n",
    "import codecs\n",
    "\n",
    "# a = \"E:\\\\1Now\\\\taglist\\\\mwlimgGrpbw.txt\" # 90 empty files\n",
    "a = \"E:\\\\1Now\\\\taglist\\\\mwlimgGrpcol.txt\" # 16 empty files\n",
    "\n",
    "outfolder = \"E:\\\\1Now\\\\taglist\\\\mwlimglist\\\\\"\n",
    "with codecs.open(a, 'r', 'utf-8') as f:\n",
    "    names = f.read().splitlines()\n",
    "    f.close()\n",
    "cnt = 0\n",
    "for name in names:\n",
    "    open(outfolder + name + '.txt', 'w')\n",
    "    cnt += 1\n",
    "print cnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "['airplane.gif.txt', 'alligator.gif.txt', 'ape.gif.txt', 'bag.gif.txt', 'baking.gif.txt', 'ball.gif.txt', 'bat.gif.txt', 'bathroom.gif.txt', 'bear.gif.txt', 'beard.gif.txt', 'bed.gif.txt', 'bicycle.gif.txt', 'bird.gif.txt', 'birds c9.jpg.txt', 'boat.gif.txt', 'brass instrument.gif.txt', 'camera.gif.txt', 'camping.gif.txt', 'carpentry.gif.txt', 'car_rev.gif.txt', 'car_types.gif.txt', 'cat.gif.txt', 'chess.gif.txt', 'clothing c13.jpg.txt', 'clothing c14.jpg.txt', 'clothing c15.jpg.txt', 'clothing c16.jpg.txt', 'colors c1.jpg.txt', 'colors c2.jpg.txt', 'colors c3.jpg.txt', 'computer.gif.txt', 'construction.gif.txt', 'deer.gif.txt', 'dog.gif.txt', 'dolphin.gif.txt', 'door.gif.txt', 'engine.gif.txt', 'eye.gif.txt', 'face.gif.txt', 'family.gif.txt', 'fan.gif.txt', 'fish and shellfish c8.jpg.txt', 'fishing.gif.txt', 'flower.gif.txt', 'font.gif.txt', 'foot.gif.txt', 'frog.gif.txt', 'fruits c5.jpg.txt', 'gardening.gif.txt', 'gems and jewelry c11.jpg.txt', 'geometry.gif.txt', 'glove.gif.txt', 'grooming.gif.txt', 'gun.gif.txt', 'gym.gif.txt', 'hair.gif.txt', 'hand.gif.txt', 'hat.gif.txt', 'hook.gif.txt', 'horse.gif.txt', 'hospital.gif.txt', 'house.gif.txt', 'human.gif.txt', 'insects and arachnids c10.jpg.txt', 'keyboard instruments.gif.txt', 'kitchen.gif.txt', 'knob.gif.txt', 'landscapes c7.jpg.txt', 'lighting.gif.txt', 'living room.gif.txt', 'llama.gif.txt', 'mail.gif.txt', 'motorcycle.gif.txt', 'mouth.gif.txt', 'nuts.gif.txt', 'office.gif.txt', 'orchestra.gif.txt', 'pasta.gif.txt', 'patterns c12.jpg.txt', 'percussion.gif.txt', 'place setting.gif.txt', 'plants c6.jpg.txt', 'playground.gif.txt', 'playing card.gif.txt', 'plow.gif.txt', 'plumbing.gif.txt', 'position.gif.txt', 'racket.gif.txt', 'rodent.gif.txt', 'scuba.gif.txt', 'sewing.gif.txt', 'ship.gif.txt', 'shoe.gif.txt', 'skate.gif.txt', 'ski.gif.txt', 'street.gif.txt', 'stringed instrument.gif.txt', 'telephone.gif.txt', 'theater.gif.txt', 'truck.gif.txt', 'vegetables c4.jpg.txt', 'windmill.gif.txt', 'window.gif.txt', 'wolf.gif.txt', 'woodwind.gif.txt', 'zodiac.gif.txt']\n"
     ]
    }
   ],
   "source": [
    "a = os.listdir(\"E:\\\\1Now\\\\taglist\\\\mwlimglist\\\\\")\n",
    "print len(a)\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\dictrawfile-test\\\n",
      "img group file \" airplane.gif.txt \":  [u'airplane', u'aileron', u'cockpit', u'fuselage', u'jet engine', u'nose', u'tail', u'wing']\n",
      "img group file \" colors c1.jpg.txt \":  [u'black', u'white', u'gray', u'yellow', u'blue', u'red', u'green', u'orange', u'purple', u'brown']\n",
      "1 --- {'prn': [\"'e~,ple!n\"], 'hw': 'airplane'}\n",
      "airplane  has a group image:  airplane.gif\n",
      "2 --- {'prn': [\"'braUn\"], 'hw': 'brown'}\n",
      "brown  has a group image:  colors c1.jpg\n",
      "3 --- {'prn': [\"'braUn!S\"], 'hw': 'brown'}\n",
      "brown  has a group image:  colors c1.jpg\n",
      "4 --- {'hw': 'brown'}\n",
      "brown  has a group image:  colors c1.jpg\n",
      "5 --- {'prn': [\"'kA:k,p!t\"], 'hw': 'cockpit'}\n",
      "cockpit  has a group image:  airplane.gif\n",
      "6 --- {'prn': [\"'g^v~m.nt\", \",g^v~n'mEntl-\", \",g^v~n'mEntl-i\"], 'hw': 'government'}\n",
      "7 --- {'prn': [\"!n'kA~n.t\"], 'hw': 'incarnate'}\n",
      "is a homograph\n",
      "8 --- {'prn': [\"!n'kA~,ne!t\"], 'hw': 'incarnate'}\n",
      "is a homograph\n",
      "9 --- {'prn': [\"'mEdl-\"], 'hw': 'medal'}\n",
      "medal  has a single image:  medal.gif\n",
      "10 --- {'hw': 'medal'}\n",
      "medal  has a single image:  medal.gif\n",
      "11 --- {'prn': [\"r!'voUlt\"], 'hw': 'revolt'}\n",
      "12 --- {'hw': 'revolt'}\n",
      "total words:  12 , French words:  0 , begin with a dash: 0 , begin with a ':  0\n"
     ]
    }
   ],
   "source": [
    "# mwaled\n",
    "#buildJSON20151109\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "import codecs\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "#connect to MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.dpdb  # database: dpdb\n",
    "\n",
    "\n",
    "#mypath = os.getcwd() + \"\\\\rawfile\"\n",
    "testflag = 1 # if it is a test: 1-yes, 0-no\n",
    "if testflag == 1:\n",
    "    mypath = \"E:\\\\dictrawfile-test\\\\\"\n",
    "    geweiDict = db.geweiDictTest\n",
    "elif testflag == 0:\n",
    "    mypath = \"E:\\\\dictrawfile-39195\\\\\"\n",
    "    geweiDict = db.geweiDict201511 # collection (table): geweiDict201511\n",
    "print mypath\n",
    "\n",
    "\n",
    "def a_has_id(tag):\n",
    "    return tag.has_attr('id')\n",
    "\n",
    "def a_has_href(tag):\n",
    "    return tag.has_attr('href')\n",
    "\n",
    "tlpath = 'E:\\\\1Now\\\\taglist\\\\'\n",
    "tfiles = ['core31.txt', 'core37.txt', 'wordRoot.txt', 'prefix.txt', 'suffix.txt',\n",
    "          'acaWriting.txt', 'airTravel.txt', 'bodyPart.txt', 'campus.txt', 'economics.txt', 'emotion.txt', \n",
    "          'employment.txt', 'environment.txt', 'family.txt', 'health.txt', 'job.txt', 'legal.txt', \n",
    "          'nonCount.txt', 'personality.txt', 'sports.txt', 'timeAdv.txt', 'timePeriod.txt', 'weather.txt']\n",
    "topics = []\n",
    "for tfile in tfiles:\n",
    "    topic = {}\n",
    "    with codecs.open('E:\\\\1Now\\\\taglist\\\\' + tfile, 'r', 'utf-8') as f:\n",
    "        twords = f.read().splitlines()\n",
    "        f.close()\n",
    "    topic['words'] = twords\n",
    "    topic['topic'] = tfile.split('.')[0]\n",
    "    topics.append(topic)\n",
    "\n",
    "    \n",
    "# homographs\n",
    "with codecs.open(tlpath + 'homog.txt', 'r', 'utf-8') as f:\n",
    "    homogs = f.read().splitlines()\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "# imgages in mwaled\n",
    "# 267 gif (black-and-white single word images)\n",
    "with codecs.open(tlpath + 'mwlimgSingle.txt', 'r', 'utf-8') as f:\n",
    "    imgSingles = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "# 16 jpg (color pages)\n",
    "# 90 gif (black-and-white collection images)\n",
    "imgLists = os.listdir(tlpath + \"mwlimglist\\\\\")\n",
    "imgGrps = []\n",
    "for imgList in imgLists:\n",
    "    imgGrp = {}\n",
    "    with codecs.open(tlpath + \"mwlimglist\\\\\" + imgList, 'r', 'utf-8') as f:\n",
    "        twords = f.read().splitlines()\n",
    "        f.close()\n",
    "    if twords:\n",
    "        print 'img group file \"', imgList, '\": ', twords\n",
    "        imgGrp['words'] = twords\n",
    "        imgGrp['img'] = imgList.split('.')[0] + '.' + imgList.split('.')[1]\n",
    "        imgGrps.append(imgGrp)\n",
    "    \n",
    "    \n",
    "labels = ('US', 'chiefly US', 'Brit', 'chiefly Brit', 'slang', 'offensive', 'obscene', 'impolite', \n",
    "          'informal', 'formal', 'literary', 'old-fashioned', 'humorous', 'technical', \n",
    "          'disapproving', 'approving', 'medical', 'law', 'baseball', 'mathematics', 'sports', \n",
    "          'grammar', 'trademark', 'old-fashioned')\n",
    "\n",
    "#map IPA image symbols to my ASCII IPA symbols\n",
    "ipa_to_alpha = {\n",
    "    '22089': 'Z', #ZH -> Z\n",
    "    '22212': 'Z', #ZH -> Z\n",
    "    '22369': 'Z', #ZH -> Z\n",
    "    '22077': 'z',\n",
    "    '22200': 'y',\n",
    "    '22047': 'w',\n",
    "    '22005': 'v',\n",
    "    '22218': 'u-Y',\n",
    "    '22167': 'u-B',\n",
    "    '22170': 'u-b',\n",
    "    '22203': 'u-b',\n",
    "    '22137': 'u-A',\n",
    "    '22140': 'u-a',\n",
    "    '22254': 'u-2',\n",
    "    '22152': 'u-1',\n",
    "    '22242': 'u-1',\n",
    "    '22263': 'u-1',\n",
    "    '22294': 'u-1',\n",
    "    '22312': 'u-1',\n",
    "    '22339': 'u-_',\n",
    "    '22185': 'u-^',\n",
    "    '22309': 'u-#',\n",
    "    '22321': 'u-#',\n",
    "    '22029': 'U', # u -> U\n",
    "    '22074': 'u',\n",
    "    '22342': 'U', # u -> U\n",
    "    '22101': 'th', # TH -> th\n",
    "    '22206': 'th', # TH -> th\n",
    "    '22366': 'th', # TH -> th\n",
    "    '22038': 't',\n",
    "    '22044': 'S', # SH -> S\n",
    "    '22209': 'S', # SH -> S\n",
    "    '22363': 'S', # SH -> S\n",
    "    '22014': 's',\n",
    "    '22050': 'r',\n",
    "    '22092': 'p',\n",
    "    '22281': 'p',\n",
    "    '22026': 'o',\n",
    "    '22068': 'N', # NG -> N\n",
    "    '22360': 'N', # NG -> N\n",
    "    '21981': 'n',\n",
    "    '22080': 'n-', # n -> n-\n",
    "    '22357': 'n-', # n -> n-\n",
    "    '22035': 'm',\n",
    "    '22227': 'm',\n",
    "    '22023': 'l',\n",
    "    '22053': 'l-', # l -> l-\n",
    "    '22354': 'l-', # l -> l-\n",
    "    '22008': 'k',\n",
    "    '22071': 'dZ', # JH -> dZ\n",
    "    '22351': 'dZ', # JH -> dZ\n",
    "    '22062': 'j',\n",
    "    '21966': '!', # i -> !\n",
    "    '22032': 'i',\n",
    "    '22333': '!', # i -> !\n",
    "    '22065': 'h',\n",
    "    '22110': 'h',\n",
    "    '22086': 'g',\n",
    "    '22083': 'f',\n",
    "    '21996': '~', # ER -> ~\n",
    "    '22266': '~', # ER -> ~\n",
    "    '22330': '~', # ER -> ~\n",
    "    '21963': 'e',\n",
    "    '22059': 'E', # e -> E\n",
    "    '22269': 'E', # e -> E\n",
    "    '22345': 'E', # e -> E\n",
    "    '22131': 'TH', # DH -> TH\n",
    "    '22116': 'dd', # no change but what 'dd' means?\n",
    "    '21999': 'd',\n",
    "    '22104': 'tS', # CH -> tS\n",
    "    '22348': 'tS', # CH -> tS\n",
    "    '22119': 'b-a',\n",
    "    '22128': 'b-a',\n",
    "    '22011': 'b',    \n",
    "    '22056': '^', # AH -> ^\n",
    "    '22327': '^', # AH -> ^\n",
    "    '21975': '.', # AH -> .\n",
    "    '22272': '.', # AH -> .\n",
    "    '22324': '.', # AH -> .  \n",
    "    '21984': '@', # AE -> @\n",
    "    '21990': 'A', # a -> A\n",
    "    '22020': 'a',\n",
    "    '22257': 'A', # a -> A\n",
    "    '22336': 'A', # a -> A\n",
    "    '22041': 'u-~', # ~ -> u-~\n",
    "    '22002': ',', # ` -> , secondary stress\n",
    "    '21993': ':', # _ -> :\n",
    "    '22260': ':', # _ -> :\n",
    "    '21960': \"'\", # ^ -> ' primary stress\n",
    "    '22149': ';',\n",
    "    '21957': '/',\n",
    "    '21978': '', # , -> null\n",
    "    '21969': '#',\n",
    "    '21972': '#',\n",
    "    '21987': '#',\n",
    "    '22017': '#',\n",
    "    '22095': '#',\n",
    "    '22098': '#',\n",
    "    '22107': '#',\n",
    "    '22113': '#',\n",
    "    '22122': '#',\n",
    "    '22125': '#',\n",
    "    '22134': '#',\n",
    "    '22143': '#',\n",
    "    '22146': '#',\n",
    "    '22155': '#',\n",
    "    '22158': '#',\n",
    "    '22161': '#',\n",
    "    '22164': '#',\n",
    "    '22173': '#',\n",
    "    '22176': '#',\n",
    "    '22179': '#',\n",
    "    '22182': '#',\n",
    "    '22188': '#',\n",
    "    '22191': '#',\n",
    "    '22194': '#',\n",
    "    '22197': '#',\n",
    "    '22215': '#',\n",
    "    '22221': '#',\n",
    "    '22224': '#',\n",
    "    '22230': '#',\n",
    "    '22233': '#',\n",
    "    '22236': '#',\n",
    "    '22239': '#',\n",
    "    '22245': '#',\n",
    "    '22248': '#',\n",
    "    '22251': '#',\n",
    "    '22275': '#',\n",
    "    '22278': '#',\n",
    "    '22284': '#',\n",
    "    '22287': '#',\n",
    "    '22291': '#',\n",
    "    '22297': '#',\n",
    "    '22300': '#',\n",
    "    '22303': '#',\n",
    "    '22306': '#',\n",
    "    '22315': '#',\n",
    "    '22318': '#',\n",
    "    '22372': '#'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "onlyfiles = [ f for f in os.listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "\n",
    "cnt = 0  #total words\n",
    "cnt_f = 0 #number of foreign words (French, Spanish)\n",
    "cnt_d = 0 #number of words beginning with a dash\n",
    "cnt_sq = 0 #number of words beginning with a single quote\n",
    "\n",
    "kk = 0\n",
    "for fname in onlyfiles:\n",
    "    jsonBody = {}\n",
    "    \n",
    "    if testflag == 0: fj = codecs.open('E:\\\\dictjson\\\\' + fname + '.json', 'w', 'utf-8')\n",
    "    #print fj\n",
    "    \n",
    "    with open(mypath + fname, 'r') as fentry:    \n",
    "    #with codecs.open('E:\\\\dictrawfile-39195\\\\' + fname, 'r', 'utf-8') as fentry: #don't work somehow\n",
    "        raw_doc = fentry.read()\n",
    "        fentry.close()\n",
    "    #fentry = open('declare.axb8472', 'r')\n",
    "    #hw_doc = fentry.read()\n",
    "        \n",
    "    \n",
    "    #get IPA symbols\n",
    "    #first find the pair of slashes used to mark the beginning and end of a pronunciation\n",
    "    #image21955.gif, image21956.gif, image21957.gif -- images of a slash '/'\n",
    "    patt = '<img hspace=\"0\" align=\"middle\" losrc=\"Images\\/image21955\\.gif\" src=\"Images\\/image21956\\.gif\" hisrc=\"Images\\/image21957\\.gif\"\\/>'\n",
    "    pattern = patt + '(.*?)' + patt\n",
    "    #print pattern\n",
    "    ipa = re.compile(pattern, re.DOTALL)\n",
    "\n",
    "    flag_brit = 0 #suppose there is no British pronunciation\n",
    "    my_pronun = []\n",
    "    for sounds in ipa.findall(raw_doc):\n",
    "        i = 0\n",
    "        for sound in sounds.split('<i>Brit</i>'):\n",
    "            i += 1\n",
    "            ipa_soup = BeautifulSoup(sound)\n",
    "            if i == 1: cmu = ''\n",
    "            elif i == 2: #'Brit'\n",
    "                flag_brit = 1 #there is British pronunciation\n",
    "                break #skip British pronunciation\n",
    "            for symbol in ipa_soup.find_all('img'):\n",
    "                key = symbol['hisrc'][12:-4]\n",
    "                if key in ipa_to_alpha: cmu += ipa_to_alpha[key]\n",
    "                else: cmu += key\n",
    "            #print cmu\n",
    "            #print \"---\\n\"   \n",
    "            my_pronun.append(cmu)\n",
    "    #print my_pronun\n",
    "    if my_pronun: jsonBody['prn'] = my_pronun\n",
    "    if flag_brit == 1: jsonBody['prn_brit'] = 'y'\n",
    "        \n",
    "    \n",
    "    #after getting IPA, remove all image tags    \n",
    "    hw_doc = re.sub(r\"(<img[^>]*\\/>)\", \"\", raw_doc)\n",
    "    #remove Unicode 'middle dot' u+00B7\n",
    "    hw_doc = re.sub(u\"\\u00B7\", \"\", hw_doc)\n",
    "    #print hw_doc\n",
    "    #continue\n",
    "    \n",
    "    #fentry.close() # move up\n",
    "\n",
    "    soup = BeautifulSoup(hw_doc)\n",
    "    \n",
    "    #get headword\n",
    "    headword = soup.find('idx:orth')\n",
    "    #if headword[0] == \"'\": headword = \"\\\\\" + headword\n",
    "    if headword: \n",
    "        hw = headword['value']\n",
    "        #jsonBody['hw'] = headword['value']\n",
    "        jsonBody['hw'] = hw\n",
    "        kk += 1\n",
    "        print kk, '---', jsonBody\n",
    "    else:\n",
    "        print fname, \": no headword\"\n",
    "        break\n",
    "        \n",
    "    \n",
    "    #check if it is in 3,000 basic English words (not accurate)\n",
    "    basic_word = soup.find('u')\n",
    "    if basic_word: jsonBody['bw'] = 1    \n",
    "    #print jsonBody\n",
    "    \n",
    "\n",
    "    #special words\n",
    "    if fname[0] == '.':\n",
    "        cnt_f += 1\n",
    "        jsonBody[\"fr\"] = \"y\" #is it a foreign word? yes\n",
    "    elif fname[0] == '-':\n",
    "        cnt_d += 1\n",
    "        jsonBody[\"dash\"] = \"y\" # begin with a dash\n",
    "    elif fname[0] == \"'\":\n",
    "        cnt_sq += 1\n",
    "        jsonBody[\"sq\"] = \"y\" #begin with a single quote '\n",
    "    \n",
    "    \n",
    "    #get inflection\n",
    "    infl = []\n",
    "    for it in soup.find_all('idx:iform'):\n",
    "        infl.append(it['value'])\n",
    "        #print infl\n",
    "    if infl: jsonBody['infl'] = infl\n",
    "\n",
    "    #get id\n",
    "    aid = soup.find(a_has_id)\n",
    "    #print type(aid)\n",
    "    if aid: jsonBody['id'] = aid['id']\n",
    "\n",
    "    #get part of speech\n",
    "    #<i><font color=\"#999999\">noun</font></i>\n",
    "    pos = soup.find(color=\"#999999\")\n",
    "    if pos: jsonBody['pos'] = pos.get_text()\n",
    "\n",
    "        \n",
    "    # if headword in homograph list, generate \"homog\" key and value    \n",
    "    homogflag = 0\n",
    "    if hw in homogs:\n",
    "        print 'is a homograph'\n",
    "        homogflag = 1\n",
    "        jsonBody['homog'] = headword['value'] + \" \" + pos.get_text() # not to append suffix, may wav or mp3\n",
    " \n",
    "    # if head word is related to images\n",
    "    hwimg = []\n",
    "    hwimgtag = []\n",
    "    for imgSig in imgSingles:\n",
    "        if hw == imgSig.split('.')[0]:\n",
    "            hwimg.append(imgSig)\n",
    "            hwimgtag.append('imgmwl')\n",
    "            print hw, ' has a single image: ', imgSig\n",
    "    for imgGrp in imgGrps:\n",
    "        if hw in imgGrp['words']:\n",
    "            hwimg.append(imgGrp['img'])\n",
    "            hwimgtag.append('imgmwlG')          \n",
    "            print hw, ' has a group image: ', imgGrp['img']\n",
    "    if hwimg:\n",
    "        jsonBody['img'] = hwimg\n",
    "        \n",
    "        \n",
    "    #get labels\n",
    "    label = []\n",
    "    label = soup.find_all(text=labels)\n",
    "    #some labels are not captured in tags, so use regex to get them\n",
    "    if soup.find(text=re.compile(\"figuratively\")): label.append(\"figuratively\")\n",
    "    if soup.find(text=re.compile(\"informal\")): label.append(\"informal\")\n",
    "    #no 'see color picture' in mobi file\n",
    "    #if soup.find_all(text=re.compile(\"see color picture\")): label.append(\"color picture\")\n",
    "    if label: jsonBody['label'] = label\n",
    "\n",
    "    #get common phrases\n",
    "    cplist = []\n",
    "    for item in soup.find_all('span'):\n",
    "        if item.parent.name == 'font': cplist.append(item.get_text())\n",
    "    if cplist: jsonBody['cp'] = cplist\n",
    "\n",
    "    #get all example sentences\n",
    "    examples = []\n",
    "    for it in soup.find_all(color=\"#002984\"):\n",
    "        examples.append(it.get_text())\n",
    "    if examples:\n",
    "        jsonBody['alles'] = examples        \n",
    "\n",
    "        totales = len(examples)        \n",
    "\n",
    "        if totales >= 100: nes = \"100+\"\n",
    "        elif totales >= 50: nes = \"50-99\"\n",
    "        elif totales >= 10: nes = \"10-49\"\n",
    "        elif totales >=  3: nes = \"3-9\"\n",
    "        elif totales >=  2: nes = \"2\"\n",
    "        elif totales >=  1: nes = \"1\"\n",
    "    else: nes = \"0\"\n",
    "        \n",
    "        \n",
    "    #get all idioms & phrasal verbs, including common phrases\n",
    "    idpvs = []\n",
    "    #get sub head words\n",
    "    subhw = []\n",
    "    for item in soup.find_all('span'):\n",
    "        #print item.parent.name, \"---\", item.get_text()\n",
    "        if item.parent.name != 'idx:entry':\n",
    "            if ' ' in item.get_text(): idpvs.append(item.get_text())\n",
    "            elif item.get_text()[0] == hw[0]: # check if the first letters of hw and subhw are same\n",
    "                subhw.append(item.get_text())\n",
    "    if idpvs: jsonBody['idpvs'] = idpvs\n",
    "    if subhw: jsonBody['subhw'] = subhw\n",
    "                \n",
    "\n",
    "    #check if head word or sub head words are in some topic word lists\n",
    "    tag = []\n",
    "    tag.append('mwl')\n",
    "    tag.append('es' + nes)    \n",
    "    for topic in topics:\n",
    "        if hw in topic['words']: tag.append(topic['topic'])\n",
    "    if homogflag == 1: tag.append('homog')\n",
    "    if hwimgtag: tag += hwimgtag\n",
    "            \n",
    "# will use mwaled77 to get a word's head word which is used to check what tags are.\n",
    "#         elif subhw:\n",
    "#             for csubhw in subhw:\n",
    "#                 if csubhw in topic['words']: tag.append(topic['topic'])\n",
    "    if tag: jsonBody['tag'] = tag\n",
    "        \n",
    "                \n",
    "    #get cross references\n",
    "    cr = {}\n",
    "    for link in soup.find_all(a_has_href):\n",
    "        if link: cr[link.get('href').replace('#','')] = link.get_text().strip()\n",
    "    if cr: jsonBody['cr'] = cr\n",
    "\n",
    "        \n",
    "    #construct definitions & examples\n",
    "    sublist = []\n",
    "    i = 1 #cross reference counter\n",
    "    \n",
    "    # remove j that is not necessary\n",
    "    #j = 1 #idioms & phrasal verbs counter \n",
    "\n",
    "    for it in soup.find_all('blockquote'):\n",
    "        #print it.parent.parent.name, \">>\", it.parent.name, \"---\", it, \"\\n\"\n",
    "        if(it.parent.name=='idx:entry'):\n",
    "            examp = []\n",
    "            defi = it.get_text()\n",
    "            if defi==\" \": continue\n",
    "            for e in it.find_all(color=\"#002984\"):\n",
    "                examp.append(e.get_text())\n",
    "                defi = defi.replace(e.get_text(),\"\")\n",
    "            #print defi\n",
    "            #print examp\n",
    "\n",
    "            subentry = {}\n",
    "            # def is a key word of Python\n",
    "            subentry['defi'] = defi\n",
    "            subentry['es'] = examp\n",
    "            #print subentry\n",
    "            #print sublist                \n",
    "\n",
    "            if subentry: sublist.append(subentry)\n",
    "        \n",
    "        #idioms & phrasal verbs\n",
    "        if(it.parent.name=='div' and it.parent.parent.name!='blockquote'):\n",
    "            subentry = {}\n",
    "            \n",
    "            #just use 'idpv' as key\n",
    "            subentry['idpv'] = it.get_text()\n",
    "            #subentry['idpv'+str(j)] = it.get_text()\n",
    "            #j += 1\n",
    " \n",
    "            #print subentry\n",
    "            if subentry: sublist.append(subentry)\n",
    "\n",
    "            \n",
    "    if sublist: jsonBody['sublist'] = sublist\n",
    "    if jsonBody:\n",
    "        #print jsonBody\n",
    "        cnt += 1\n",
    "        \n",
    "        #dump json into a file\n",
    "        if testflag == 0: json.dump(jsonBody, fj, indent=4, sort_keys=True)\n",
    "        \n",
    "        # insert into MongoDB database\n",
    "        doc_id = geweiDict.insert(jsonBody)\n",
    "        #print doc_id \n",
    "        if testflag == 0: shutil.move(mypath + fname, \"E:\\\\dictprocessed\\\\\"+fname)\n",
    "    if testflag == 0: fj.close()\n",
    "\n",
    "print 'total words: ', cnt, ', French words: ', cnt_f, \", begin with a dash:\", cnt_d, \", begin with a ': \", cnt_sq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "ta = \"a, b, c\"\n",
    "tb = 'dd'\n",
    "print ta.split(',')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acaWriting.txt', 'airTravel.txt', 'bodyPart.txt', 'campus.txt', 'economics.txt', 'emotion.txt', 'employment.txt', 'environment.txt', 'family.txt', 'health.txt', 'job.txt', 'legal.txt', 'list.txt', 'nonCount.txt', 'personality.txt', 'sport.txt', 'timeAdv.txt', 'timePeriod.txt', 'weather.txt']\n"
     ]
    }
   ],
   "source": [
    "with open('core3000\\\\topic.txt', 'r') as f:\n",
    "    lines = f.read().splitlines()\n",
    "print lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "tfiles = ['acaWriting.txt', 'airTravel.txt', 'bodyPart.txt', 'campus.txt', 'economics.txt', 'emotion.txt', \n",
    "          'employment.txt', 'environment.txt', 'family.txt', 'health.txt', 'job.txt', 'legal.txt', \n",
    "          'nonCount.txt', 'personality.txt', 'sport.txt', 'timeAdv.txt', 'timePeriod.txt', 'weather.txt']\n",
    "topics = []\n",
    "cnt = 0\n",
    "for tfile in tfiles:\n",
    "    topic = {}\n",
    "    with codecs.open('core3000\\\\' + tfile, 'r', 'utf-8') as f:\n",
    "        twords = f.read().splitlines()\n",
    "        cnt += len(twords)\n",
    "        f.close()\n",
    "    topic['words'] = twords\n",
    "    topic['topic'] = tfile.split('.')[0]\n",
    "    topics.append(topic)\n",
    "#print topics    \n",
    "print cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "fcore = codecs.open('core3000\\\\core3140.txt', 'w', 'utf-8')\n",
    "with codecs.open('core3000\\\\core3000raw20151123.txt', 'r', 'utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "cnt = 0    \n",
    "uCore = set()\n",
    "\n",
    "for line in lines:    \n",
    "    uCore.add(line.split('\\t')[0])\n",
    "uCoreSorted = sorted(uCore)    \n",
    "for word in uCoreSorted:\n",
    "    cnt += 1\n",
    "    fcore.write(word+'\\n')\n",
    "    #print cnt, ': ', word\n",
    "fcore.close()    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'decompose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a6d700773372>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'decompose'"
     ]
    }
   ],
   "source": [
    "print(soup.img.decompose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpdict.find({\"label\":\"mathematics\"}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpdict.find({\"label\":\"slang\"}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpdict.find({\"label\":\"offensive\"}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpdict.find({\"label\":\"chiefly US\"}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2615"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpdict.find({\"label\":\"Brit\"}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2027"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.dpdict.find({\"label\":\"US\"}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-208-11f5d97b9c3b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-208-11f5d97b9c3b>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    db.dpdict.find({cp: {$exists: true}}).count()\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "db.dpdict.find({cp: {$exists: true}}).count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'_id': ObjectId('537e93cd8707ee3128063ada'),\n",
       " u'alles': [u'I saw your doppelg\\xe4nger [=(more commonly) double] yesterday.',\n",
       "  u'In the story, the character is haunted by a doppelg\\xe4nger.'],\n",
       " u'fr': u'y',\n",
       " u'hw': u'doppelg\\xe4nger',\n",
       " u'idpvs': [u'dop\\xb7pel\\xb7gang\\xb7er'],\n",
       " u'infl': {u'1np1': u'doppelg\\xe4ngers', u'1ns1': u'doppelg\\xe4nger'},\n",
       " u'label': [u'literary'],\n",
       " u'pos': u'or',\n",
       " u'sublist': [{u'def': u'1 : someone who looks like someone else  ',\n",
       "   u'es': [u'I saw your doppelg\\xe4nger [=(more commonly) double] yesterday.']},\n",
       "  {u'def': u'2 literary : a ghost that looks like a living person ',\n",
       "   u'es': [u'In the story, the character is haunted by a doppelg\\xe4nger.']}]}"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpdict.find_one({'hw': u'doppelg\\xe4nger'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.cursor.Cursor at 0x7993630>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpdict.find({\"id\":\"filepos18005907\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img hspace=\"0\" align=\"middle\" losrc=\"Images\\/image21955\\.gif\" src=\"Images\\/image21956\\.gif\" hisrc=\"Images\\/image21957\\.gif\"\\/>(.*?)<img hspace=\"0\" align=\"middle\" losrc=\"Images\\/image21955\\.gif\" src=\"Images\\/image21956\\.gif\" hisrc=\"Images\\/image21957\\.gif\"\\/>\n",
      "---\n",
      "\n",
      "Images/image21960.gif\n",
      "Images/image22005.gif\n",
      "Images/image21984.gif\n",
      "Images/image22014.gif\n",
      "Images/image22038.gif\n",
      "Images/image21978.gif\n",
      "---\n",
      "\n",
      "Images/image21960.gif\n",
      "Images/image22005.gif\n",
      "Images/image21990.gif\n",
      "Images/image21993.gif\n",
      "Images/image22014.gif\n",
      "Images/image22038.gif\n",
      "---\n",
      "\n",
      "Images/image21960.gif\n",
      "Images/image22005.gif\n",
      "Images/image21984.gif\n",
      "Images/image22014.gif\n",
      "Images/image22038.gif\n",
      "Images/image21981.gif\n",
      "Images/image21975.gif\n",
      "Images/image22014.gif\n",
      "Images/image21978.gif\n",
      "---\n",
      "\n",
      "Images/image21960.gif\n",
      "Images/image22005.gif\n",
      "Images/image21990.gif\n",
      "Images/image21993.gif\n",
      "Images/image22014.gif\n",
      "Images/image22038.gif\n",
      "Images/image21981.gif\n",
      "Images/image21975.gif\n",
      "Images/image22014.gif\n"
     ]
    }
   ],
   "source": [
    "comment = re.compile(r'/\\*(.*?)\\*/', re.DOTALL)\n",
    "patt = '<img hspace=\"0\" align=\"middle\" losrc=\"Images\\/image21955\\.gif\" src=\"Images\\/image21956\\.gif\" hisrc=\"Images\\/image21957\\.gif\"\\/>'\n",
    "pattern = patt + '(.*?)' + patt\n",
    "print pattern\n",
    "ipa = re.compile(pattern, re.DOTALL)\n",
    "text1 = '/* this is a comment */'\n",
    "text2 = '''/* this is a\n",
    "        multiline comment */\n",
    "        '''\n",
    "text3 = '''<idx:entry scriptable=\"yes\"><idx:orth value=\"vast\"><idx:infl>  <idx:iform name=\"1.adj.pos.1\" value=\"vast\"/></idx:infl><idx:infl>  <idx:iform name=\"2.n.s.1\" value=\"vast\"/>  <idx:iform name=\"2.n.p.1\" value=\"vasts\"/></idx:infl></idx:orth><a id=\"filepos83514877\" /><span><font size=\"3\"><b><u>vast</u></span> <img hspace=\"0\" align=\"middle\" losrc=\"Images/image21955.gif\" src=\"Images/image21956.gif\" hisrc=\"Images/image21957.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image21958.gif\" src=\"Images/image21959.gif\" hisrc=\"Images/image21960.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image22003.gif\" src=\"Images/image22004.gif\" hisrc=\"Images/image22005.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image21982.gif\" src=\"Images/image21983.gif\" hisrc=\"Images/image21984.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image22012.gif\" src=\"Images/image22013.gif\" hisrc=\"Images/image22014.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image22036.gif\" src=\"Images/image22037.gif\" hisrc=\"Images/image22038.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image21976.gif\" src=\"Images/image21977.gif\" hisrc=\"Images/image21978.gif\"/> <i>Brit</i> <img hspace=\"0\" align=\"middle\" losrc=\"Images/image21958.gif\" src=\"Images/image21959.gif\" hisrc=\"Images/image21960.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image22003.gif\" src=\"Images/image22004.gif\" hisrc=\"Images/image22005.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image21988.gif\" src=\"Images/image21989.gif\" hisrc=\"Images/image21990.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image21991.gif\" src=\"Images/image21992.gif\" hisrc=\"Images/image21993.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image22012.gif\" src=\"Images/image22013.gif\" hisrc=\"Images/image22014.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image22036.gif\" src=\"Images/image22037.gif\" hisrc=\"Images/image22038.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image21955.gif\" src=\"Images/image21956.gif\" hisrc=\"Images/image21957.gif\"/> <i><font color=\"#999999\">adj</font></i>, <b>vast積r</b>, <b>-est</b> [<i>more <img hspace=\"0\" align=\"middle\" losrc=\"Images/image22039.gif\" src=\"Images/image22040.gif\" hisrc=\"Images/image22041.gif\"/>; most <img hspace=\"0\" align=\"middle\" losrc=\"Images/image22039.gif\" src=\"Images/image22040.gif\" hisrc=\"Images/image22041.gif\"/></i>]<blockquote align=\"left\"> <b>:</b> very great in size, amount, or extent <blockquote align=\"left\"><font color=\"#002984\">She has a <i>vast</i> amount of knowledge on this subject.</font></blockquote> <blockquote align=\"left\"><font color=\"#002984\"><i>vast</i> quantities of information</font></blockquote> <blockquote align=\"left\"><font color=\"#002984\">The policy is supported by the <i>vast</i> majority of citizens.</font></blockquote> <blockquote align=\"left\"><font color=\"#002984\">a <i>vast</i> expanse of land</font></blockquote></blockquote> <img hspace=\"0\" vspace=\"0\" align=\"middle\" src=\"Images/image21970.gif\"/><div align=\"left\"><blockquote><span><font size=\"3\"><b>vast穕y</span> <i><font color=\"#999999\">adv</font></i> <blockquote><font color=\"#002984\">His background is <i>vastly</i> different from mine.</font></blockquote> <blockquote><font color=\"#002984\">They <i>vastly</i> increased spending.</font></blockquote></blockquote> </div>\n",
    "\n",
    "<img hspace=\"0\" vspace=\"0\" align=\"middle\" src=\"Images/image21970.gif\"/><div align=\"left\"><blockquote><span><font size=\"3\"><b>vast穘ess</span> \n",
    "\n",
    "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image21955.gif\" src=\"Images/image21956.gif\" hisrc=\"Images/image21957.gif\"/>\n",
    "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image21958.gif\" src=\"Images/image21959.gif\" hisrc=\"Images/image21960.gif\"/>\n",
    "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image22003.gif\" src=\"Images/image22004.gif\" hisrc=\"Images/image22005.gif\"/>\n",
    "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image21982.gif\" src=\"Images/image21983.gif\" hisrc=\"Images/image21984.gif\"/>\n",
    "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image22012.gif\" src=\"Images/image22013.gif\" hisrc=\"Images/image22014.gif\"/>\n",
    "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image22036.gif\" src=\"Images/image22037.gif\" hisrc=\"Images/image22038.gif\"/>\n",
    "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image21979.gif\" src=\"Images/image21980.gif\" hisrc=\"Images/image21981.gif\"/>\n",
    "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image21973.gif\" src=\"Images/image21974.gif\" hisrc=\"Images/image21975.gif\"/>\n",
    "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image22012.gif\" src=\"Images/image22013.gif\" hisrc=\"Images/image22014.gif\"/>\n",
    "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image21976.gif\" src=\"Images/image21977.gif\" hisrc=\"Images/image21978.gif\"/>\n",
    " <i>Brit</i> \n",
    " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image21958.gif\" src=\"Images/image21959.gif\" hisrc=\"Images/image21960.gif\"/>\n",
    " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image22003.gif\" src=\"Images/image22004.gif\" hisrc=\"Images/image22005.gif\"/>\n",
    " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image21988.gif\" src=\"Images/image21989.gif\" hisrc=\"Images/image21990.gif\"/>\n",
    " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image21991.gif\" src=\"Images/image21992.gif\" hisrc=\"Images/image21993.gif\"/>\n",
    " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image22012.gif\" src=\"Images/image22013.gif\" hisrc=\"Images/image22014.gif\"/>\n",
    " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image22036.gif\" src=\"Images/image22037.gif\" hisrc=\"Images/image22038.gif\"/>\n",
    " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image21979.gif\" src=\"Images/image21980.gif\" hisrc=\"Images/image21981.gif\"/>\n",
    " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image21973.gif\" src=\"Images/image21974.gif\" hisrc=\"Images/image21975.gif\"/>\n",
    " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image22012.gif\" src=\"Images/image22013.gif\" hisrc=\"Images/image22014.gif\"/>\n",
    "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image21955.gif\" src=\"Images/image21956.gif\" hisrc=\"Images/image21957.gif\"/> \n",
    "\n",
    "\n",
    "<i><font color=\"#999999\">noun</font></i> [<i>noncount</i>] <blockquote><font color=\"#002984\">the <i>vastness</i> of the desert/ocean</font></blockquote></blockquote></div></idx:entry><div><img hspace=\"0\" vspace=\"0\" align=\"middle\" src=\"Images/image21971.gif\"/></div><div><table width=\"100%\" bgcolor=\"#7593CD\"><tr><th widht=\"100%\" height=\"2px\"/></tr></table></div><div><img hspace=\"0\" vspace=\"0\" align=\"middle\" src=\"Images/image21972.gif\"/></div> \n",
    " \n",
    " '''\n",
    "#comment.findall(text1)\n",
    "#comment.findall(text2)\n",
    "for sounds in ipa.findall(text3):\n",
    "    for sound in sounds.split('<i>Brit</i>'):\n",
    "        print \"---\\n\"\n",
    "        soup = BeautifulSoup(sound)\n",
    "        for symbol in soup.find_all('img'):\n",
    "            print symbol['hisrc']\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vowels\n",
    "\n",
    "@  æ ask bat glad\n",
    "\n",
    "a:   ɑ: cot bomb caught paw\n",
    "\n",
    "e    ɛ bet fed\n",
    "\n",
    ".    ə about banana collide\n",
    "\n",
    "i  i very any thirty\n",
    "\n",
    "i:   i: eat bead bee\n",
    "\n",
    "!    ɪ id bid pit\n",
    "\n",
    "u   ʊ foot should put\n",
    "\n",
    "u:  u: boot two coo\n",
    "\n",
    "^   ʌ under putt bud\n",
    "\n",
    "~   ɚ merge bird further\n",
    "\n",
    "ei   eɪ eight wade bay\n",
    "\n",
    "ai   aɪ ice bite tie\n",
    "\n",
    "au   aʊ out gown plow\n",
    "\n",
    "oi    oɪ oyster coil boy\n",
    "\n",
    "ou   oʊ oat own zone blow\n",
    "\n",
    "a~  ɑɚ car heart bizarre\n",
    "\n",
    "e~  eɚ bare fair wear\n",
    "\n",
    "i~   iɚ near deer mere pier\n",
    "\n",
    "o~   oɚ boar port door shore\n",
    "\n",
    "u~   uɚ boor tour insure\n",
    "\n",
    "\n",
    "Consonants\n",
    "\n",
    "b   b baby labor cab\n",
    "\n",
    "d   d day kid\n",
    "\n",
    "dZ   ʤ just badger fudge\n",
    "\n",
    "TH  ð then either bathe\n",
    "\n",
    "f   f foe tough buff\n",
    "\n",
    "g   g go dagger bag\n",
    "\n",
    "h   h hot ahead\n",
    "\n",
    "j   j yes vineyard\n",
    "\n",
    "k   k lacquer flock skin\n",
    "\n",
    "l   l law hollow\n",
    "\n",
    "l-   l̟ pedal battle final\n",
    "\n",
    "m   m mat hemp hammer rim\n",
    "\n",
    "n   n new tent tenor run\n",
    "\n",
    "n-   n̩ button satin kitten\n",
    "\n",
    "N   ŋ rung hang swinger\n",
    "\n",
    "p   p lapse top lip speed\n",
    "\n",
    "r   r rope arrive\n",
    "\n",
    "s   s sad mist kiss\n",
    "\n",
    "S   ʃ shoe mission slush\n",
    "\n",
    "t   t mat stick late\n",
    "\n",
    "(tʰ toe attack)\n",
    "\n",
    "(ɾ later catty riddle)\n",
    "\n",
    "tS   tʃ batch nature\n",
    "\n",
    "(tʃʰ choose chin achieve)\n",
    "\n",
    "th   θ thin ether bath\n",
    "\n",
    "v   v vat never cave\n",
    "\n",
    "w   w wet software\n",
    "\n",
    "z   z zoo easy buzz\n",
    "\n",
    "Z   ʒ vision azure beige\n",
    "\n",
    "Other Symbols\n",
    "\n",
    "'   ' penmanship\n",
    ",  ˌ penmanship\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "#connect to MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.dpdb\n",
    "dpdict = db.dpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'His outburst seemed incongruous to those who know him well.', u'The style of the porch is incongruous with [=does not match] the style of the house overall.', u'The modern sculpture seems incongruous [=out of place] among all the antiques.']\n",
      "[{u'def': u' : strange because of not agreeing with what is usual or expected   ', u'es': [u'His outburst seemed incongruous to those who know him well.', u'The style of the porch is incongruous with [=does not match] the style of the house overall.', u'The modern sculpture seems incongruous [=out of place] among all the antiques.']}, {u'idpv1': u'in\\xb7con\\xb7gru\\xb7i\\xb7ty noun, pl -ties [count, noncount]'}, {u'idpv2': u'in\\xb7con\\xb7gru\\xb7ous\\xb7ly adv'}]\n"
     ]
    }
   ],
   "source": [
    "words = dpdict.find({\"hw\":\"incongruous\"})\n",
    "for word in words:\n",
    "    print word.get(\"alles\")\n",
    "    print word.get(\"sublist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
