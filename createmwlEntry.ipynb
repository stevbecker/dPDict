{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk\n",
      "Awol\n",
      "-ness\n",
      "February\n",
      "<type 'str'>\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print 'walk'.capitalize() # Walk is good\n",
    "print 'AWOL'.capitalize() # not good because Awol isn't a word\n",
    "print '-ness'.capitalize()\n",
    "print 'February'.capitalize()\n",
    "al = string.lowercase[0:26]\n",
    "print type(al)\n",
    "# for x in al: print x\n",
    "print 'Tuesday'[0] in al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total head words:  39195\n",
      "hw unique number:  34398\n",
      "39195\n",
      "number of hws with homographs:  39195\n",
      "number of unique hws:  34398\n",
      "number of words capitalized:  37517\n",
      "67125\n"
     ]
    }
   ],
   "source": [
    "# output mwl39195hw.txt  (having duplicates)\n",
    "# and explore 39195 hw entries\n",
    "# 2015-12-01\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# codecs if for read and write files with unicode, encoding 'utf-8'\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "#connect to MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.dpdb  # databas: dpdb\n",
    "geweiDict = db.geweiDict201511 # collection (table): geweiDict201511\n",
    "#geweiDict = db.geweiDictTest\n",
    "\n",
    "fhw = codecs.open('E:\\\\1Now\\\\taglist\\\\mwl39195hwraw.txt', 'w', 'utf-8')\n",
    "    \n",
    "results = geweiDict.find({}, {\"_id\":0, \"hw\":1}) #.limit(1000)\n",
    "\n",
    "\n",
    "totalhw = 0 # total number of head words\n",
    "for result in results:    \n",
    "    word = result.get(\"hw\")    \n",
    "    if word:\n",
    "        #print word\n",
    "        fhw.write(word + '\\n')\n",
    "        totalhw += 1\n",
    "    else:\n",
    "        print \"no head word\"\n",
    "        continue\n",
    "\n",
    "fhw.close()\n",
    "print \"total head words: \", totalhw\n",
    "\n",
    "\n",
    "# sort out by head words and output\n",
    "# with homographs, 39195 head words have duplicates; unique number is 34398\n",
    "\n",
    "# create mwl39195hw.txt\n",
    "with codecs.open('E:\\\\1Now\\\\taglist\\\\mwl39195hwraw.txt', 'r', 'utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "lines = sorted(lines)\n",
    "print 'hw unique number: ', len(set(lines))\n",
    "\n",
    "fd = codecs.open('E:\\\\1Now\\\\taglist\\\\mwl39195hw.txt', 'w', 'utf-8')\n",
    "cnt = 0\n",
    "for line in lines:\n",
    "    fd.write(line + '\\n')\n",
    "    cnt += 1\n",
    "fd.close()\n",
    "print cnt\n",
    "\n",
    "\n",
    "\n",
    "# explore 39195 hws\n",
    "with codecs.open('E:\\\\1Now\\\\taglist\\\\mwl39195hw.txt', 'r', 'utf-8') as f:\n",
    "    words = f.read().splitlines()\n",
    "    f.close()\n",
    "print 'number of hws with homographs: ', len(words)\n",
    "print 'number of unique hws: ', len(set(words))\n",
    "\n",
    "# wordsC = [x.capitalize() for x in words]\n",
    "wordsC = []\n",
    "for word in words:\n",
    "    # only capitalize words beginning with a lowercase letter. no need for 'Feburary'\n",
    "    # Don't capitalize words like 'AWOL' -- Awol is not a word!\n",
    "    if word[0] in string.lowercase[0:26]:\n",
    "        wordsC.append(word.capitalize())\n",
    "        cnt += 1\n",
    "    \n",
    "print 'number of words capitalized: ', len(wordsC)\n",
    "\n",
    "# put words and wordsC together and get unique\n",
    "# wordsTotal = sorted(set(words + wordsC))\n",
    "# fd = codecs.open('E:\\\\1Now\\\\taglist\\\\mwl39195hwCap.txt', 'w', 'utf-8')\n",
    "# for x in wordsTotal:\n",
    "#     fd.write(x + '\\n')\n",
    "# fd.close()\n",
    "# print len(wordsTotal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total head words:  39195\n",
      "hw unique number:  34398\n",
      "39195\n",
      "hw summary unique number:  38862\n",
      "39195\n"
     ]
    }
   ],
   "source": [
    "# output mwl39195sum.txt  (having duplicates)\n",
    "# hw entries and their summary (subhw, prn, idpvs, tags)\n",
    "# 2015-11-27 2015-12-01\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# codecs if for read and write files with unicode, encoding 'utf-8'\n",
    "import codecs\n",
    "\n",
    "#connect to MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.dpdb  # databas: dpdb\n",
    "geweiDict = db.geweiDict201511 # collection (table): geweiDict201511\n",
    "#geweiDict = db.geweiDictTest\n",
    "\n",
    "fdict = codecs.open('E:\\\\1Now\\\\taglist\\\\mwl39195sumraw.txt', 'w', 'utf-8')\n",
    "    \n",
    "results = geweiDict.find({}, {\"_id\":0, \"hw\":1, \"subhw\":1, \"prn\":1, \"idpvs\":1, \"tag\":1}) #.limit(1000)\n",
    "\n",
    "\n",
    "totalhw = 0 # total number of head words\n",
    "for result in results:    \n",
    "    word = result.get(\"hw\")    \n",
    "    if word:\n",
    "        #print word\n",
    "        fdict.write(word)\n",
    "        totalhw += 1\n",
    "    else:\n",
    "        print \"no head word\"\n",
    "        continue\n",
    "\n",
    "    # get items of result\n",
    "    prns = result.get(\"prn\")\n",
    "    subhws = result.get(\"subhw\")\n",
    "    idpvs = result.get(\"idpvs\")  \n",
    "    tags = result.get('tag')\n",
    "    \n",
    "    if subhws:\n",
    "        cnt = 1\n",
    "        for subhw in subhws:\n",
    "            if cnt == 1: fdict.write(' (' + subhw)                \n",
    "            else: fdict.write(' ' + subhw)\n",
    "            cnt += 1        \n",
    "        fdict.write(')')\n",
    "    \n",
    "    if prns:\n",
    "        cnt = 1\n",
    "        for prn in prns:                 \n",
    "            fdict.write(' /' + prn + '/')\n",
    "            cnt += 1\n",
    "\n",
    "    if idpvs:\n",
    "        cnt = 1\n",
    "        for idpv in idpvs:\n",
    "            if cnt == 1: fdict.write('  [' + idpv)\n",
    "            else: fdict.write(', ' + idpv)\n",
    "            cnt += 1\n",
    "        fdict.write(']')\n",
    "            \n",
    "    if tags:\n",
    "        cnt = 1\n",
    "        for tag in tags:\n",
    "            if cnt == 1: fdict.write('  {' + tag)\n",
    "            else: fdict.write(' ' + tag)\n",
    "            cnt += 1\n",
    "        fdict.write('}')\n",
    "\n",
    "    fdict.write('\\n')    \n",
    "\n",
    "fhw.close()\n",
    "fdict.close()\n",
    "print \"total head words: \", totalhw\n",
    "\n",
    "\n",
    "# sort out and create mwl39195sum.txt\n",
    "with codecs.open('E:\\\\1Now\\\\taglist\\\\mwl39195sumraw.txt', 'r', 'utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "lines = sorted(lines)\n",
    "print 'hw summary unique number: ', len(set(lines))\n",
    "\n",
    "fd = codecs.open('E:\\\\1Now\\\\taglist\\\\mwl39195sum.txt', 'w', 'utf-8')\n",
    "cnt = 0\n",
    "for line in lines:\n",
    "    fd.write(line + '\\n')\n",
    "    cnt += 1\n",
    "fd.close()\n",
    "print cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- mwBen statistics ---\n",
      "total head words:  39195\n",
      "total inflections:  64355\n",
      "total subhws:  9544\n",
      "total subhws whose 1st letters are different from hws:  334\n",
      "only hw without derivatives:  11182\n",
      "number of lines in mwBen:  53972\n"
     ]
    }
   ],
   "source": [
    "# output mwlBenchmark.txt (mwBen) (no duplicates)\n",
    "\n",
    "# format: derivatives of hw (inflections, or subhws);hw\n",
    "# sort by 'derivatives;hw' and make every line unique\n",
    "# 2015-11-28, 2015-11-30, 2015-12-01\n",
    "\n",
    "# blazes;blaze\n",
    "# blazing;blaze\n",
    "# blazingly;blazing\n",
    "\n",
    "# I can know blaze, blazing are hws, the two lines below are not allowd because hws have derivates:\n",
    "# blaze;blaze\n",
    "# blazing;blazing\n",
    "# the two lines above are redundant.\n",
    "\n",
    "# manually delete: asas\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# codecs if for read and write files with unicode, encoding 'utf-8'\n",
    "import codecs\n",
    "\n",
    "#connect to MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.dpdb  # databas: dpdb\n",
    "geweiDict = db.geweiDict201511 # collection (table): geweiDict201511\n",
    "#geweiDict = db.geweiDictTest\n",
    "\n",
    "fdict = codecs.open('E:\\\\1Now\\\\taglist\\\\mwlBenchmarkraw.txt', 'w', 'utf-8')\n",
    "    \n",
    "results = geweiDict.find({}, {\"_id\":0, \"hw\":1, \"infl\":1, \"subhw\":1}) #.limit(1000)\n",
    "# sort method will use much more memory\n",
    "#results = geweiDict.find({}, {\"_id\":0, \"hw\":1, \"prn\":1, \"alles\":1, \"idpvs\":1, \"sublist\":1}).sort(\"hw\", 1).limit(1000)\n",
    "\n",
    "\n",
    "hwcnt = 0 # number of head words (hws)\n",
    "# hwcapcnt = 0 # number of hws capitalized\n",
    "inflcnt = 0 # number of inflections\n",
    "subhwcnt = 0 # number of subhws\n",
    "subhw1cnt = 0 # number of subhws whose first letter is not same as hw\n",
    "onlyhwcnt = 0\n",
    "\n",
    "# subhw1hw = []\n",
    "\n",
    "for result in results:\n",
    "    inflflag = 0\n",
    "    subhwflag = 0\n",
    "    \n",
    "    word = result.get(\"hw\")    \n",
    "    if word:\n",
    "        #print word\n",
    "#         fdict.write(word + ';' + word + '\\n')\n",
    "        hwcnt += 1      \n",
    "    else:\n",
    "        print \"no head word\"\n",
    "        continue\n",
    "\n",
    "    # get items of result\n",
    "    infls = result.get(\"infl\")\n",
    "    subhws = result.get(\"subhw\")     \n",
    "    \n",
    "    if infls:\n",
    "        for infl in infls:\n",
    "            if infl == word:\n",
    "#                 print 'infl == word', infl\n",
    "                continue\n",
    "            fdict.write(infl + ';' + word + '\\n')\n",
    "            inflcnt += 1   \n",
    "            inflflag = 1\n",
    "            \n",
    "    if subhws:\n",
    "        for subhw in subhws:\n",
    "            #if subhw[0] != word[0]: continue       \n",
    "            if subhw == word:\n",
    "#                 print 'subhw == word', infl\n",
    "                continue           \n",
    "            if subhw[0] != word[0]:\n",
    "#                 print 'subhw is not like word: ', subhw, '...', word\n",
    "                subhw1cnt += 1\n",
    "#                 subhw1hw.append(subhw+';'+word)\n",
    "            fdict.write(subhw + ';' + word + '\\n')\n",
    "            subhwcnt += 1      \n",
    "            subhwflag = 1\n",
    "    if inflflag == 0 and subhwflag == 0:\n",
    "        fdict.write(word + ';' + word + '\\n')\n",
    "        onlyhwcnt += 1\n",
    "    \n",
    "fdict.close()\n",
    "\n",
    "print '--- mwBen statistics ---'\n",
    "print \"total head words: \", hwcnt\n",
    "# print 'total hws capitalized: ', hwcapcnt\n",
    "print \"total inflections: \", inflcnt\n",
    "print 'total subhws: ', subhwcnt\n",
    "print 'total subhws whose 1st letters are different from hws: ', subhw1cnt\n",
    "print 'only hw without derivatives: ', onlyhwcnt\n",
    "\n",
    "\n",
    "# sort out by head words\n",
    "with codecs.open('E:\\\\1Now\\\\taglist\\\\mwlBenchmarkraw.txt', 'r', 'utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "lines = sorted(set(lines))\n",
    "\n",
    "fd = codecs.open('E:\\\\1Now\\\\taglist\\\\mwlBenchmark.txt', 'w', 'utf-8')\n",
    "cnt = 0\n",
    "for line in lines:\n",
    "    fd.write(line + '\\n')\n",
    "    cnt += 1\n",
    "fd.close()\n",
    "print 'number of lines in mwBen: ', cnt\n",
    "\n",
    "# subhw1hw = sorted(set(subhw1hw))\n",
    "# fd = codecs.open('E:\\\\1Now\\\\taglist\\\\subhw1hw.txt', 'w', 'utf-8')\n",
    "# for x in subhw1hw:\n",
    "#     fd.write(x + '\\n')\n",
    "# fd.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCDEFGHIJKLMNOPQRSTUVWXYZ\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print string.uppercase[0:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- explore mwBen ---\n",
      "number of derivative column:  53972\n",
      "number of hw column:  53972\n",
      "unique number of hw column:  34398\n",
      "number of derivatives (inflections and subhws) whose 1st letter different from hws:  337\n",
      "number of capitalized hws:  1487\n"
     ]
    }
   ],
   "source": [
    "# explore mwBen\n",
    "\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "with codecs.open('E:\\\\1Now\\\\taglist\\\\mwlBenchmark.txt', 'r', 'utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "deriv = [] # derivative\n",
    "benhw = [] # hw\n",
    "deriv1hw = [] # derivative (inflection, subhw) first letter different from its hw\n",
    "hwcap = [] # hw whose first letter is capitalized\n",
    "for line in lines:\n",
    "    x1 = line.split(';')[0]\n",
    "    x2 = line.split(';')[1]\n",
    "    deriv.append(x1)\n",
    "    benhw.append(x2)\n",
    "    if x1[0] != x2[0]: deriv1hw.append(line)\n",
    "    if x2[0] in string.uppercase[0:26]: hwcap.append(x2)\n",
    "derivS = sorted(deriv)\n",
    "benhwSU = sorted(set(benhw))\n",
    "hwcap = sorted(set(hwcap))\n",
    "\n",
    "print '--- explore mwBen ---'\n",
    "print 'number of derivative column: ', len(deriv)\n",
    "print 'number of hw column: ', len(benhw)\n",
    "print 'unique number of hw column: ', len(benhwSU)\n",
    "print 'number of derivatives (inflections and subhws) whose 1st letter different from hws: ', len(deriv1hw)\n",
    "print 'number of capitalized hws: ', len(hwcap)\n",
    "\n",
    "fout = codecs.open('E:\\\\1Now\\\\taglist\\\\mwlStat.txt', 'w', 'utf-8')\n",
    "fout.write('--- statistics ---\\n')\n",
    "fout.write('number of lines in mwBen: ' + str(len(deriv)) + '\\n')\n",
    "fout.write('unique number of hw: ' + str(len(benhwSU)) + '\\n')\n",
    "fout.write('number of derivatives (inflections and subhws) whose 1st letter different from hws: ' + str(len(deriv1hw)) + '\\n')\n",
    "fout.write('number of capitalized hws: ' + str(len(hwcap)) + '\\n')\n",
    "\n",
    "fout.write('Contents\\n--------\\n')\n",
    "fout.write(\"Q: Are there duplicate derivatives who aren't hws in mwBen?\\n\")\n",
    "fout.write(\"Q: List derivatives (inflections and subhws) whose first letter is different from the first letter of their hws\\n\")\n",
    "fout.write(\"Capitalized hws\\n\\n\")\n",
    "\n",
    "# Q: Are there duplicate derivatives who aren't hws in mwBen?\n",
    "dupderiv = [x for x in derivS if x not in benhwSU and derivS.count(x)>1]\n",
    "fout.write(\"--- Duplicate derivatives not being hws: ---\\n\")\n",
    "for x in dupderiv:\n",
    "    fout.write(x + '\\n')\n",
    "\n",
    "# Q: List subhws whose first letter is different from the first letter of their hws\n",
    "fout.write(\"\\n--- derivatives (inflections and subhws) whose 1st letter different from hws: ---\\n\")\n",
    "fout.write(\"derivatives (inflections and subhws);hw\\n\\n\")\n",
    "for x in subhw1hw:\n",
    "    fout.write(x + '\\n')\n",
    "\n",
    "fout.write(\"\\n--- Capitalized hws: ---\\n\")\n",
    "for x in hwcap:\n",
    "    fout.write(x + '\\n')\n",
    "           \n",
    "fout.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 4, 5}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get duplicates\n",
    "l = [1,2,3,4,4,5,5,6,1]\n",
    "set([x for x in l if l.count(x) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3905 3140\n",
      "661\n",
      "set([])\n"
     ]
    }
   ],
   "source": [
    "# output core31.txt by processing core3000raw20151123.txt\n",
    "\n",
    "with codecs.open('E:\\\\1Now\\\\taglist\\\\core3000raw20151123.txt', 'r', 'utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "words = [x.split('\\t')[0] for x in lines]\n",
    "wordsu = sorted(set(words))\n",
    "fout = codecs.open('E:\\\\1Now\\\\taglist\\\\core3140n.txt', 'w', 'utf-8')\n",
    "for word in wordsu:\n",
    "    fout.write(word + '\\n')\n",
    "fout.close()\n",
    "\n",
    "print len(words), len(wordsu)\n",
    "dupl = set([x for x in words if words.count(x)>1])\n",
    "print len(dupl)\n",
    "# print sorted(dupl)\n",
    "\n",
    "# # compare core3140n.txt and core31.txt\n",
    "# with codecs.open('E:\\\\1Now\\\\taglist\\\\core31.txt', 'r', 'utf-8') as f:\n",
    "#     core31s = f.read().splitlines()\n",
    "#     f.close()\n",
    "# print set(core31s) ^ set(wordsu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello ... hello2\n",
      "hi ... hi2\n"
     ]
    }
   ],
   "source": [
    "def gethw():\n",
    "    a = [{'k1': 'hello', 'k2':'hello2'}, {'k1': 'hi', 'k2':'hi2'}]\n",
    "    return a\n",
    "b = gethw()\n",
    "for x in b:\n",
    "    print x['k1'], '...', x['k2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ core3140n.txt ------\n",
      "* has words :  3140\n",
      "* has unique word number:  3140\n",
      "* has duplicates:  set([])\n",
      "\n",
      "\n",
      "derivative:  annoying --- hw:  annoy\n",
      "derivative:  annually --- hw:  annual\n",
      "derivative:  constantly --- hw:  constant\n",
      "derivative:  contamination --- hw:  contaminate\n",
      "derivative:  continually --- hw:  continual\n",
      "derivative:  continuously --- hw:  continuous\n",
      "derivative:  customs --- hw:  custom\n",
      "derivative:  damages --- hw:  damage\n",
      "derivative:  dietician --- hw:  dietitian\n",
      "derivative:  doubles --- hw:  double\n",
      "derivative:  extroverted --- hw:  extrovert\n",
      "derivative:  flurries --- hw:  flurry\n",
      "derivative:  frequently --- hw:  frequent\n",
      "derivative:  impatience --- hw:  impatient\n",
      "derivative:  infrequently --- hw:  infrequent\n",
      "derivative:  introverted --- hw:  introvert\n",
      "derivative:  loneliness --- hw:  lonely\n",
      "derivative:  mediation --- hw:  mediate\n",
      "derivative:  normally --- hw:  normal\n",
      "derivative:  notes --- hw:  note\n",
      "derivative:  previously --- hw:  previous\n",
      "derivative:  sadness --- hw:  sad\n",
      "derivative:  savings --- hw:  saving\n",
      "derivative:  singles --- hw:  single\n",
      "derivative:  unhappiness --- hw:  unhappy\n",
      "\n",
      "\n",
      "* same as hw:  3111\n",
      "* same as capitalized hw:  0\n",
      "* words are derivatives:  25\n",
      "* not in mwBen:  4  --  [u'earlier', u'half time', u'semi-annually', u'sportmanship']\n",
      "--- len(txtraw) == len(wordhw) + len(wordhwC) + len(wordderiv) + len(wordnotBen) ---\n",
      "\n",
      "how many duplicates in raw hwreturn ---  26  -  13  =  13\n",
      "[u'annoy', u'annoy', u'annual', u'annual', u'constant', u'constant', u'continuous', u'continuous', u'damage', u'damage', u'double', u'double', u'impatient', u'impatient', u'lonely', u'lonely', u'normal', u'normal', u'previous', u'previous', u'sad', u'sad', u'single', u'single', u'unhappy', u'unhappy']\n",
      "\n",
      "how many hws supposed to be returned---  3123\n",
      "acutal return:  3123\n",
      "3123\n"
     ]
    }
   ],
   "source": [
    "# check word lists only for building mwl mongodb database\n",
    "# these word lists include (number in parentheses are the number of files)\n",
    "#   * words of 18 topics from mwl website (18)\n",
    "#   * 3140 core words from mwl website (1)\n",
    "#   * words of \"The Spelling of Different Sounds in English\" (1)\n",
    "#   * words of word roots, prefixes, suffixes (3)\n",
    "#   sub-total: 23\n",
    "#\n",
    "#   * words of mwl single images (1)\n",
    "#   * words of mwl group images (106)\n",
    "#   sub-total: 107\n",
    "#\n",
    "#   * homographs (1)\n",
    "#   * 3787 core words I collect (1)\n",
    "#   sub-total: 2\n",
    "# \n",
    "#   Total: 131\n",
    "\n",
    "\n",
    "\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def checkBymwBen(path, wordlist):\n",
    "    '''get sorted unique head words of mwl for a word list which may include inflections and/or sub hws'''\n",
    "    \n",
    "    hwreturn = [] # hws returned by this function    \n",
    "    \n",
    "    # word list I want to check\n",
    "    with codecs.open(path + wordlist, 'r', 'utf-8') as f:\n",
    "        txtraw = f.read().splitlines()\n",
    "    print '------', wordlist, '------'\n",
    "    print '* has words : ', len(txtraw)\n",
    "    print '* has unique word number: ', len(set(txtraw))\n",
    "    print '* has duplicates: ', set([x for x in txtraw if txtraw.count(x) > 1])\n",
    "    print '\\n'\n",
    "\n",
    "    # with codecs.open(\"E:\\\\1Now\\\\taglist\\\\mwl77Entry.txt\", 'r', 'utf-8') as f:\n",
    "    with codecs.open(\"E:\\\\1Now\\\\taglist\\\\mwlBenchmark.txt\", 'r', 'utf-8') as f:    \n",
    "        lines = f.read().splitlines()\n",
    "        f.close()\n",
    "\n",
    "    # create mwBen dictionary\n",
    "    mwBens = []\n",
    "    hws = [] # sorted unique hws\n",
    "    hwsC = [] # sorted unique capitalized hws\n",
    "    for line in lines:\n",
    "        entry = {}\n",
    "        hw = line.split(';')[1]\n",
    "        entry['hw'] = hw\n",
    "        entry['deriv'] = line.split(';')[0] # derivative, or hw that has no derivatives\n",
    "        mwBens.append(entry)\n",
    "        hws.append(hw)\n",
    "        if hw[0] in string.lowercase[0:26]:\n",
    "            hwsC.append(hw.capitalize())\n",
    "        \n",
    "    hws = sorted(set(hws))\n",
    "    hwsC = sorted(set(hwsC))        \n",
    "\n",
    "#     print 'mwBens: ', len(mwBens)\n",
    "#     print 'hws: ', len(hws)\n",
    "#     print 'hwsC: ', len(hwsC)\n",
    "\n",
    "\n",
    "    # begin to check the word list against mw benchmark dictionary\n",
    "    #  Word; Its-hw (Head Word)\n",
    "    #\n",
    "    # Three cases (the first two cases return hw)\n",
    "    # 1. word is exactly same as hw or is a capitalized hw\n",
    "    # 2. word is a derivative or is a capitalized derivative\n",
    "    # 3. word is not in mwBen\n",
    "\n",
    "\n",
    "    wordhw = [] # word is exactly same as hw\n",
    "    wordhwC = [] # word is same as capitalized hw\n",
    "    wordderiv = [] # derivatives or capitalized derivatives\n",
    "    wordnotBen = [] # word is not in mwBen\n",
    "\n",
    "    for word in txtraw:\n",
    "        hwflag = 0\n",
    "        derivflag = 0\n",
    "        \n",
    "        if word in hws:\n",
    "            wordhw.append(word)\n",
    "            hwreturn.append(word)\n",
    "            hwflag = 1\n",
    "        elif word in hwsC:\n",
    "            wordhwC.append(word)\n",
    "            hwreturn.append(word.lower())\n",
    "            hwflag = 1\n",
    "\n",
    "        if hwflag == 0:\n",
    "            dercnt = 0\n",
    "            for mwben in mwBens:                \n",
    "                if word == mwben['deriv'] or word == mwben['deriv'].capitalize():\n",
    "                    wordderiv.append(word)\n",
    "                    hwreturn.append(mwben['hw'])\n",
    "                    derivflag = 1\n",
    "                    print 'derivative: ', word, '--- hw: ', mwben['hw']\n",
    "                    dercnt += 1\n",
    "            if dercnt > 1: print word, '--- derivate has multiple hws --- ', dercnt\n",
    "\n",
    "        if hwflag == 0 and derivflag == 0:\n",
    "            wordnotBen.append(word)\n",
    "    \n",
    "    print '\\n'\n",
    "    print '* same as hw: ', len(wordhw)  #,' -- ', wordhw\n",
    "    print '* same as capitalized hw: ', len(wordhwC)  #,' -- ', wordhwC\n",
    "    print '* words are derivatives: ', len(wordderiv) #,' -- ', wordderiv\n",
    "    print '* not in mwBen: ', len(wordnotBen), ' -- ', wordnotBen\n",
    "    if len(txtraw) == len(wordhw) + len(wordhwC) + len(wordderiv) + len(wordnotBen): \n",
    "        print '--- len(txtraw) == len(wordhw) + len(wordhwC) + len(wordderiv) + len(wordnotBen) ---\\n'\n",
    "    else: print \"--- words' numbers don't mattch, check ---\\n\"\n",
    "    \n",
    "    dupl = [x for x in hwreturn if hwreturn.count(x)>1]\n",
    "    print 'how many duplicates in raw hwreturn --- ', len(dupl), ' - ', len(set(dupl)), ' = ', len(dupl)-len(set(dupl))\n",
    "    print dupl\n",
    "\n",
    "    print '\\nhow many hws supposed to be returned--- ', len(txtraw) - len(wordnotBen) - (len(dupl)-len(set(dupl)))\n",
    "    hwreturn = sorted(set(hwreturn))\n",
    "    print 'acutal return: ', len(hwreturn)\n",
    "    return hwreturn\n",
    "    \n",
    "    \n",
    "result = checkBymwBen(\"E:\\\\1Now\\\\taglist\\\\originaltaglist\\\\\", \"core3140n.txt\")\n",
    "# result = checkBymwBen('E:\\\\1Now\\\\taglist\\\\', 'testmwBen.txt')\n",
    "\n",
    "print len(result)\n",
    "# print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def compareWordlist(path, wl1, wl2):\n",
    "    '''get common and different parts of two word lists'''\n",
    "    with codecs.open(path + wl1, 'r', 'utf-8') as f:\n",
    "        words1 = f.read().splitlines()\n",
    "    with codecs.open(path + wl2, 'r', 'utf-8') as f:\n",
    "        words2 = f.read().splitlines()\n",
    "    f = codecs.open('E:\\\\1Now\\\\taglist\\\\compare\\\\' + wl1 + '_' + wl2 + '.txt', 'w', 'utf-8')\n",
    "    comm = [x for x in words1 if x in words2]\n",
    "    only1 = [x for x in words1 if x not in words2]\n",
    "    only2 = [x for x in words2 if x not in words1]\n",
    "    \n",
    "    f.write('words only in ' + wl1 + ': ' + str(len(only1)) + '\\n')\n",
    "    for x in only1:\n",
    "        f.write(x + '\\n')    \n",
    "    \n",
    "    f.write('\\n\\nwords only in ' + wl2 + ': ' + str(len(only2)) + '\\n')\n",
    "    for x in only2:\n",
    "        f.write(x + '\\n')       \n",
    "    \n",
    "    f.write('\\n\\ncommon words of ' + wl1 + ' and ' + wl2 + ': ' + str(len(comm)) + '\\n')\n",
    "    for x in comm:\n",
    "        f.write(x + '\\n')\n",
    "\n",
    "compareWordlist(\"E:\\\\1Now\\\\taglist\\\\originaltaglist\\\\\", \"core3140n.txt\", \"core37.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-f6fe266591e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hi'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hello'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\codecs.pyc\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwritelines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\codecs.pyc\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, object)\u001b[0m\n\u001b[0;32m    356\u001b[0m         \"\"\"\n\u001b[0;32m    357\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwritelines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: I/O operation on closed file"
     ]
    }
   ],
   "source": [
    "\n",
    "# is it necessary to close file in with statement?\n",
    "with codecs.open('E:\\\\temp\\\\with.txt', 'w', 'utf-8') as f:\n",
    "    f.write('hi')\n",
    "    f.close() # no need, it'll be closed automatically\n",
    "f.write('hello') # f has been closed automatically\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
