{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* has words :  405\n",
      "* has unique word number:  405\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  \n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  --\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  11\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  365\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  5\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  730\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  730th\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  Boston\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  PBs\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  Philadelphia\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  Philly\n",
      "derivative:  according --- hw:  accord\n",
      "derivative:  accumulated --- hw:  accumulate\n",
      "derivative:  achieved --- hw:  achieve\n",
      "derivative:  babying --- hw:  baby\n",
      "derivative:  begins --- hw:  begin\n",
      "derivative:  belies --- hw:  belie\n",
      "derivative:  bodies --- hw:  body\n",
      "derivative:  bolstered --- hw:  bolster\n",
      "derivative:  bones --- hw:  bone\n",
      "derivative:  busting --- hw:  bust\n",
      "derivative:  changed --- hw:  change\n",
      "derivative:  changes --- hw:  change\n",
      "derivative:  continues --- hw:  continue\n",
      "derivative:  crafting --- hw:  craft\n",
      "derivative:  cramps --- hw:  cramp\n",
      "derivative:  edges --- hw:  edge\n",
      "derivative:  embraced --- hw:  embrace\n",
      "derivative:  evolving --- hw:  evolve\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  faster\n",
      "derivative:  feels --- hw:  feel\n",
      "derivative:  fitness --- hw:  fit\n",
      "derivative:  follows --- hw:  follow\n",
      "derivative:  forcing --- hw:  force\n",
      "derivative:  friends --- hw:  friend\n",
      "derivative:  fuels --- hw:  fuel\n",
      "derivative:  gains --- hw:  gain\n",
      "derivative:  goals --- hw:  goal\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  harder\n",
      "derivative:  increases --- hw:  increase\n",
      "derivative:  jumping --- hw:  jump\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  kilometres\n",
      "derivative:  leaping --- hw:  leap\n",
      "derivative:  legs --- hw:  leg\n",
      "derivative:  lungs --- hw:  lung\n",
      "derivative:  meted --- hw:  mete\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  mid-summer\n",
      "derivative:  moments --- hw:  moment\n",
      "derivative:  months --- hw:  month\n",
      "derivative:  muscles --- hw:  muscle\n",
      "derivative:  needs --- hw:  need\n",
      "derivative:  pains --- hw:  pain\n",
      "derivative:  passed --- hw:  pass\n",
      "derivative:  peering --- hw:  peer\n",
      "derivative:  pushing --- hw:  push\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  reignites\n",
      "derivative:  rekindled --- hw:  rekindle\n",
      "derivative:  rekindling --- hw:  rekindle\n",
      "derivative:  relinquishes --- hw:  relinquish\n",
      "derivative:  reluctantly --- hw:  reluctant\n",
      "derivative:  reminded --- hw:  remind\n",
      "derivative:  resets --- hw:  reset\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  runner's\n",
      "derivative:  runs --- hw:  run\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  shockwave\n",
      "derivative:  shoes --- hw:  shoe\n",
      "derivative:  sparked --- hw:  spark\n",
      "derivative:  starting --- hw:  start\n",
      "derivative:  steps --- hw:  step\n",
      "derivative:  streaks --- hw:  streak\n",
      "derivative:  stretching --- hw:  stretch\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  stronger\n",
      "derivative:  tackling --- hw:  tackle\n",
      "derivative:  taking --- hw:  take\n",
      "derivative:  things --- hw:  thing\n",
      "derivative:  trails --- hw:  trail\n",
      "derivative:  travels --- hw:  travel\n",
      "derivative:  trials --- hw:  trial\n",
      "derivative:  veins --- hw:  vein\n",
      "derivative:  wanted --- hw:  want\n",
      "derivative:  warmed --- hw:  warm\n",
      "derivative:  watching --- hw:  watch\n",
      "derivative:  years --- hw:  year\n",
      "* same as hw:  286\n",
      "* capitalized hw:  36\n",
      "* words are derivatives:  64\n",
      "\n",
      "**************##########***************** All not in mwBen:  19 \n",
      "[u'', u'--', u'11', u'365', u'5', u'730', u'730th', u'Boston', u'PBs', u'Philadelphia', u'Philly', u'faster', u'harder', u'kilometres', u'mid-summer', u'reignites', u\"runner's\", u'shockwave', u'stronger']\n",
      "\n",
      "---words to learn---:  65\n",
      "deck:sPlay hw:\"I'd\" or hw:\"I'm\" or hw:\"I've\" or hw:\"November\" or hw:\"accord\" or hw:\"accumulate\" or hw:\"asphalt\" or hw:\"belie\" or hw:\"bolster\" or hw:\"bucket\" or hw:\"bust\" or hw:\"confront\" or hw:\"cramp\" or hw:\"dangle\" or hw:\"darkness\" or hw:\"dawn\" or hw:\"efficiency\" or hw:\"eight\" or hw:\"embrace\" or hw:\"evolve\" or hw:\"exhaustion\" or hw:\"five\" or hw:\"four\" or hw:\"hamstring\" or hw:\"impulse\" or hw:\"it's\" or hw:\"lap\" or hw:\"leap\" or hw:\"lightly\" or hw:\"m\" or hw:\"marathon\" or hw:\"mete\" or hw:\"mug\" or hw:\"niggle\" or hw:\"oxygen\" or hw:\"peer\" or hw:\"perpetual\" or hw:\"perspective\" or hw:\"rekindle\" or hw:\"relinquish\" or hw:\"reluctant\" or hw:\"repetition\" or hw:\"reset\" or hw:\"ritual\" or hw:\"runner\" or hw:\"simply\" or hw:\"solidify\" or hw:\"spark\" or hw:\"stoop\" or hw:\"streak\" or hw:\"stride\" or hw:\"summit\" or hw:\"tackle\" or hw:\"they're\" or hw:\"those\" or hw:\"tighten\" or hw:\"tinge\" or hw:\"truly\" or hw:\"two\" or hw:\"unexpected\" or hw:\"unintended\" or hw:\"vein\" or hw:\"workout\" or hw:\"you're\" or hw:\"you've\"\n"
     ]
    }
   ],
   "source": [
    "# tuneBymwBen(wlist=[], path='', wfile='')\n",
    "# tuneReport(wlist=[], path='', wfile='')\n",
    "# txt2wl(path='', wfile='')\n",
    "# hw4learn(wlist=[], path='', wfile='')\n",
    "# compareWordlist(l1=[], l2=[], path='', f1='', f2='')\n",
    "\n",
    "\n",
    "# tuneBymwBen(path='E:\\\\1Now\\\\taglist\\\\', wfile='testmwBen.txt') \n",
    "# tuneReport(path='E:\\\\1Now\\\\taglist\\\\', wfile='testmwBen.txt')\n",
    "\n",
    "### A typcial usage example: create words to learn Anki search string from an article\n",
    "wlraw = txt2wl('E:\\\\1Now\\\\mae\\\\article\\\\', 'runblog1.txt')\n",
    "rnt = tuneReport(wlraw)\n",
    "hw4learn(wlist=rnt['alltunehw'])\n",
    "\n",
    "\n",
    "# compareWordlist(path=\"E:\\\\1Now\\\\taglist\\\\\", f1=\"84en_fr.txt\", f2=\"mwlBenchmark.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words :  65\n",
      "65\n",
      "number of notes in Anki:  311\n"
     ]
    }
   ],
   "source": [
    "# check number of words and notes in a filtered deck\n",
    "# first export the filted deck in Anki\n",
    "\n",
    "import codecs\n",
    "with codecs.open('E:\\\\1Now\\\\mae\\\\article\\\\filter_runblog1.txt', 'r', 'utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "words = [x.split('\\t')[4] for x in lines]\n",
    "words = sorted(words)\n",
    "print 'number of unique words : ', len(set(words))\n",
    "\n",
    "b = {}\n",
    "for x in words:     \n",
    "    b[x] = words.count(x)\n",
    "#     print x, b[x]\n",
    "print len(b)\n",
    "\n",
    "cnt = 0\n",
    "for x in b:\n",
    "    #print x, b[x]\n",
    "    cnt += b[x]\n",
    "print 'number of notes in Anki: ', cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- load 5 functions ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "\n",
    "def tuneBymwBen(wlist=[], path='', wfile=''):        \n",
    "    \n",
    "    if wfile:\n",
    "        print '\\n\\n------', path+wfile, '------'\n",
    "        with codecs.open(path+wfile, 'r', 'utf-8') as f:\n",
    "            txtraw = f.read().splitlines()\n",
    "        txtraw = [x.strip() for x in txtraw]\n",
    "    elif wlist:\n",
    "        txtraw = [x.strip() for x in wlist]\n",
    "    else:\n",
    "        print 'function argumenents error'\n",
    "        return -1\n",
    "    \n",
    "    print '* has words : ', len(txtraw)\n",
    "    print '* has unique word number: ', len(set(txtraw))\n",
    "    dupinraw = set([x for x in txtraw if txtraw.count(x) > 1])\n",
    "    if dupinraw: print '* has duplicates: ', dupinraw\n",
    "   \n",
    "        \n",
    "    with codecs.open(\"E:\\\\1Now\\\\taglist\\\\mwlBenchmark.txt\", 'r', 'utf-8') as f:    \n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    # create mwBen dictionary\n",
    "    mwBens = []\n",
    "    hws = [] # sorted unique hws\n",
    "    hwsC = [] # sorted unique capitalized hws\n",
    "    for line in lines:\n",
    "        if len(line) < 1:\n",
    "            print 'len(line)<1', line \n",
    "            return -1\n",
    "        entry = {}\n",
    "        hw = line.split(';')[1]\n",
    "        entry['hw'] = hw\n",
    "        entry['deriv'] = line.split(';')[0] # derivative, or hw that has no derivatives\n",
    "        mwBens.append(entry)\n",
    "        hws.append(hw)\n",
    "        if hw[0] in string.lowercase[0:26]:\n",
    "            hwsC.append(hw.capitalize())\n",
    "        \n",
    "    hws = sorted(set(hws))\n",
    "    hwsC = sorted(set(hwsC))        \n",
    "\n",
    "\n",
    "    # begin to check the word list against mw benchmark dictionary\n",
    "    #  Word; Its-hw (Head Word)\n",
    "    #\n",
    "    # Three cases\n",
    "    # 1. word is exactly same as hw or is a capitalized hw (like 'Walk') or word.capitalize() is in hw (like 'leo')\n",
    "    # 2. word is a derivative or is a capitalized derivative (like 'Walks') or word.capitalize() is in derivate (like 'leos')\n",
    "    # 3. word is not in mwBen\n",
    "\n",
    "    \n",
    "    hwreturn = [] # hws returned by this function\n",
    "    \n",
    "    wordhw = [] # word is exactly same as hw\n",
    "    wordhwL = [] # word.lower() is a hw, like 'Walk'\n",
    "    wordhwC = [] # word.capitalize() is a hw, like 'leo'\n",
    "    \n",
    "    wordDeriv = [] # raw derivatives or capitalized derivatives or derivative.lower()\n",
    "    fineDeriv = [] # fine derivateivs are in mwBen\n",
    "    hwofDeriv = [] # hws of derivatives\n",
    "    wordnotBen = [] # word is not in mwBen\n",
    "\n",
    "    for word in txtraw:\n",
    "        hwflag = 0 # suppose that word is not a hw\n",
    "        derivflag = 0 # suppose that word is not a derivative\n",
    "        \n",
    "        # first check if word is a hw\n",
    "        if word in hws:\n",
    "            wordhw.append(word)\n",
    "            hwreturn.append(word)\n",
    "            hwflag = 1\n",
    "        elif word.capitalize() in hws: # like leo\n",
    "            wordhwC.append(word)\n",
    "            hwreturn.append(word.capitalize())\n",
    "            hwflag = 1            \n",
    "        elif word.capitalize() in hwsC: # like Walk, tAke\n",
    "            wordhwL.append(word)\n",
    "            hwreturn.append(word.lower())\n",
    "            hwflag = 1\n",
    "\n",
    "        # if word isn't a hw, then check if it's a derivative\n",
    "        if hwflag == 0:\n",
    "            dercnt = 0\n",
    "            for mwben in mwBens:                \n",
    "                if word == mwben['deriv'] or word == mwben['deriv'].capitalize() or word.capitalize() == mwben['deriv']:\n",
    "                    wordDeriv.append(word)\n",
    "                    fineDeriv.append(mwben['deriv'])\n",
    "                    hwofDeriv.append(mwben['hw'])\n",
    "                    derivflag = 1\n",
    "                    print 'derivative: ', word, '--- hw: ', mwben['hw']\n",
    "                    dercnt += 1\n",
    "            if dercnt > 1: print word, '--- derivate has multiple hws --- ', dercnt\n",
    "\n",
    "        if hwflag == 0 and derivflag == 0:\n",
    "            if word.find(' ') == -1:\n",
    "                wordnotBen.append(word)\n",
    "                print '**************^^^^^^^!!@@@@@*************** is not in mwBen: ', word\n",
    "            else:\n",
    "                print '>>> >>> >>> >>> >> not a single word: ', word\n",
    "                for x in word.split(' '):\n",
    "                    if x in hws: hwreturn.append(x)\n",
    "                    elif x.capitalize() in hws: hwreturn.append(x.capitalize())\n",
    "                    elif x in hwsC: hwreturn.append(x.lower())\n",
    "                    else:\n",
    "                        wdflagx = 0 \n",
    "                        for mwben in mwBens:                \n",
    "                            if x == mwben['deriv'] or x == mwben['deriv'].capitalize() \\\n",
    "                            or x.capitalize() == mwben['deriv']:\n",
    "                                wordDeriv.append(word)\n",
    "                                fineDeriv.append(mwben['deriv'])\n",
    "                                hwofDeriv.append(mwben['hw'])\n",
    "                                wdflagx = 1\n",
    "                        if wdflagx == 0:\n",
    "                            wordnotBen.append(x)\n",
    "                            print '**************^^^^^^^!!@@@@@*************** really is not in mwBen: ', x\n",
    "                                \n",
    "                            \n",
    "            \n",
    "    \n",
    "    if wordhw: print '* same as hw: ', len(wordhw)  #,' -- ', wordhw\n",
    "    if wordhwL: print '* capitalized hw: ', len(wordhwL)  #,' -- ', wordhwL\n",
    "    if wordhwC: print '* hw typo, should be capitalized: ', len(wordhwC)  #,' -- ', wordhwC\n",
    "    \n",
    "    if wordDeriv: print '* words are derivatives: ', len(wordDeriv) #,' -- ', wordderiv\n",
    "\n",
    "    if wordnotBen:\n",
    "        print '\\n**************##########***************** All not in mwBen: ', len(wordnotBen), '\\n', wordnotBen\n",
    "        \n",
    "#     if len(txtraw) == len(wordhw) + len(wordhwL) + len(wordhwC) + len(wordDeriv) + len(wordnotBen):         \n",
    "#     else: print \"--- words' numbers don't mattch, check ---\\n\"\n",
    "\n",
    "        \n",
    "    rntdic = {}\n",
    "    rntdic['alltunehw'] = sorted(set(hwreturn + hwofDeriv))   \n",
    "    rntdic['tunederiv'] = sorted(set(fineDeriv))\n",
    "    rntdic['rawhwderiv'] = sorted(set(wordhw + wordhwL + wordhwC + wordDeriv))\n",
    "    rntdic['notinben'] = sorted(set(wordnotBen))\n",
    "    \n",
    "    return rntdic\n",
    "    \n",
    "\n",
    "\n",
    "def tuneReport(wlist=[], path='', wfile=''):\n",
    "    if wlist: fname = 'tuneReport-list.txt'\n",
    "    elif wfile: fname = 'tuneReport-' + wfile\n",
    "    else:\n",
    "        print 'arguments error'\n",
    "        return -1\n",
    "    \n",
    "    rnt = tuneBymwBen(wlist, path, wfile)\n",
    "\n",
    "    alltunehw = rnt['alltunehw'] \n",
    "    tunederiv = rnt['tunederiv']    \n",
    "    notinben = rnt['notinben']\n",
    "    #rawhwderiv = rnt['rawhwderiv']\n",
    "    \n",
    "    fd = codecs.open('E:\\\\1Now\\\\taglist\\\\report\\\\' + fname, 'w', 'utf-8')\n",
    "    if notinben:\n",
    "        fd.write('---not in mwBen---:' + str(len(notinben)) + '\\n')\n",
    "        for x in notinben: fd.write(x + '\\n')\n",
    "    if alltunehw:\n",
    "        fd.write('\\n\\n--- all tuned hws of raw hws and derivatives which are in mwBen---: ' + str(len(alltunehw)) + '\\n')\n",
    "        for x in alltunehw: fd.write(x + '\\n')    \n",
    "    if tunederiv:\n",
    "        fd.write('\\n\\n--- all tuned derivatives which are in mwBen---: ' + str(len(tunederiv)) + '\\n')\n",
    "        for x in tunederiv: fd.write(x + '\\n')    \n",
    "\n",
    "    fd.close()\n",
    "    return rnt\n",
    "       \n",
    "\n",
    "def txt2wl(path='', wfile=''):\n",
    "    '''extract a word list from a text, return a sorted unqiue raw word list'''\n",
    "    if wfile:\n",
    "        with codecs.open(path + wfile, 'r', 'utf-8') as f:\n",
    "            parags = f.read().splitlines() # each paragraph as one line\n",
    "    else:\n",
    "        print 'argument error'\n",
    "        return -1\n",
    "    \n",
    "    wlraw = [] # raw word list from the text\n",
    "    for parag in parags:\n",
    "        regxp = re.compile(r'[^\\w\\'\\-]+', re.U) # regular expression pattern: ^\\w\\'\\- means not words ' -\n",
    "        parag = regxp.sub(' ', parag) # replace all not words ' - with a space\n",
    "        wlraw += parag.split(' ')\n",
    "    \n",
    "    return sorted(set(wlraw))\n",
    "\n",
    "\n",
    "\n",
    "def hw4learn(wlist=[], path='', wfile='', excluf='E:\\\\1Now\\\\taglist\\\\2939exclu.txt'):\n",
    "    '''This function only excludes hws and their derivatives exactly in mwl.\n",
    "    So it's better to use tuneBymwBen() first to get a fine word list'''\n",
    "    \n",
    "    if wfile:\n",
    "        with codecs.open(path + wfile, 'r', 'utf-8') as f:\n",
    "            words = f.read().splitlines()\n",
    "        words = [x.strip() for x in words]\n",
    "    elif wlist:\n",
    "        words = [x.strip() for x in wlist]\n",
    "    else:\n",
    "        print 'argument error'\n",
    "        return -1\n",
    "        \n",
    "    with codecs.open(excluf, 'r', 'utf-8') as f:\n",
    "        bexclus = f.read().splitlines()\n",
    "    \n",
    "    word4learn = []\n",
    "    for word in words:\n",
    "        if word not in bexclus: word4learn.append(word)\n",
    "    \n",
    "    word4learn = sorted(set(word4learn))\n",
    "    \n",
    "    # generate Anki search string for filtered deck    \n",
    "    ankiSearchStr = 'deck:sPlay'\n",
    "    for x in word4learn:\n",
    "        if x != word4learn[-1]: ankiSearchStr += ' hw:\"' + x + '\" or'\n",
    "        else: ankiSearchStr += ' hw:\"' + x + '\"'\n",
    "    print '\\n---words to learn---: ', len(word4learn)\n",
    "    print ankiSearchStr\n",
    "    \n",
    "#     return word4learn\n",
    "\n",
    "\n",
    "def compareWordlist(l1=[], l2=[], path='', f1='', f2=''):\n",
    "    '''get common and different parts of two lists or word lists of two files'''\n",
    "    \n",
    "    if l1:\n",
    "        words1 = l1\n",
    "#         words1 = [x.strip() for x in l1]\n",
    "        f1name = 'list-1'\n",
    "    if l2:\n",
    "        words2 = l2\n",
    "#         words2 = [x.strip() for x in l2]\n",
    "        f2name = 'list-2'\n",
    "    if f1:\n",
    "        with codecs.open(path+f1, 'r', 'utf-8') as f:\n",
    "            words1 = f.read().splitlines()\n",
    "#         words1 = [x.strip() for x in words1]\n",
    "        f1name = f1\n",
    "    if f2:\n",
    "        with codecs.open(path+f2, 'r', 'utf-8') as f:\n",
    "            words2 = f.read().splitlines()\n",
    "#         words2 = [x.strip() for x in words2]\n",
    "        f2name = f2\n",
    "            \n",
    "    comm = [x for x in words1 if x in words2]\n",
    "    only1 = [x for x in words1 if x not in words2]\n",
    "    only2 = [x for x in words2 if x not in words1]\n",
    "    \n",
    "    f = codecs.open('E:\\\\1Now\\\\taglist\\\\report\\\\compare_' + f1name + '_' + f2name + '.txt', 'w', 'utf-8')\n",
    "    \n",
    "    print '---common words of ' + f1name + ' and ' + f2name + '---: ' + str(len(comm))\n",
    "    f.write('---common words of ' + f1name + ' and ' + f2name + '---: ' + str(len(comm)) + '\\n')\n",
    "    for x in comm:\n",
    "        f.write(x + '\\n')\n",
    "    \n",
    "    print '---words only in ' + f1name + '---: ' + str(len(only1))\n",
    "    f.write('\\n\\n---words only in ' + f1name + '---: ' + str(len(only1)) + '\\n')\n",
    "    for x in only1:\n",
    "        f.write(x + '\\n')    \n",
    "    \n",
    "    print '---words only in ' + f2name + '---: ' + str(len(only2))\n",
    "    f.write('\\n\\n---words only in ' + f2name + '---: ' + str(len(only2)) + '\\n')\n",
    "    for x in only2:\n",
    "        f.write(x + '\\n')\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "    \n",
    "print '--- load 5 functions ---'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of excluded hw:  2939\n",
      "total head words:  5011\n",
      "total words:  31600\n",
      "unique words: 9833\n"
     ]
    }
   ],
   "source": [
    "# Organize 'beautifulExcluHw.txt' and generate 'beautifulExclu.txt'\n",
    "# (note: words in beautifulExcluHw.txt are absolutely hws in 39195 of mwl)\n",
    "# directly from mwaled mongodb dpdict. words not being a hw in mwaled won't be added to the two lists\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# codecs if for read and write files with unicode, encoding 'utf-8'\n",
    "import codecs\n",
    "\n",
    "#connect to MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.dpdb  # databas: dpdb\n",
    "geweiDict = db.geweiDict201511 # collection (table): geweiDict201511\n",
    "#geweiDict = db.geweiDictTest\n",
    "\n",
    "\n",
    "with codecs.open(\"E:\\\\1Now\\\\taglist\\\\2939bw.txt\", 'r', 'utf-8') as f:\n",
    "    behws = f.read().splitlines()\n",
    "    f.close()    \n",
    "# sort and make unique words in beautifulExcluHw.txt\n",
    "behws = sorted(set(behws))\n",
    "print 'number of excluded hw: ', len(behws)\n",
    "\n",
    "# fd = codecs.open(\"E:\\\\1Now\\\\mae\\\\beautifulExcluHw.txt\", 'w', 'utf-8')\n",
    "# for behw in behws:\n",
    "#     fd.write(behw + '\\n')\n",
    "# fd.close()\n",
    "    \n",
    "fdict = codecs.open('E:\\\\1Now\\\\mae\\\\beautifulExcluraw.txt', 'w', 'utf-8')\n",
    "    \n",
    "results = geweiDict.find({}, {\"_id\":0, \"hw\":1, \"infl\":1, \"subhw\":1}) #.limit(1000)\n",
    "\n",
    "# total number of head words\n",
    "totalhw = 0\n",
    "totalcnt = 0\n",
    "\n",
    "for result in results:    \n",
    "    word = result.get(\"hw\")    \n",
    "\n",
    "    if word and word in behws:\n",
    "        #print word\n",
    "#         fdict.write(word + ';' + word + '\\n')\n",
    "        fdict.write(word + '\\n')\n",
    "        totalhw += 1\n",
    "    else:\n",
    "#         print \"no head word\"\n",
    "        continue\n",
    "\n",
    "    # get items of result\n",
    "    infls = result.get(\"infl\")\n",
    "    subhws = result.get(\"subhw\")     \n",
    "    \n",
    "    if infls:\n",
    "        for infl in infls:\n",
    "#             fdict.write(infl + ';' + word + '\\n')\n",
    "            fdict.write(infl + '\\n')\n",
    "            totalcnt += 1   \n",
    "            \n",
    "    if subhws:\n",
    "        for subhw in subhws:\n",
    "            if subhw[0] != word[0]: continue # check if subhw is a real sub hw of word by comparing first letter\n",
    "#             fdict.write(subhw + ';' + word  + '\\n')\n",
    "            fdict.write(subhw + '\\n')\n",
    "            totalcnt += 1        \n",
    "    \n",
    "fdict.close()\n",
    "print \"total head words: \", totalhw\n",
    "print \"total words: \", totalcnt + totalhw\n",
    "\n",
    "\n",
    "# sort out by head words and make them unique\n",
    "with codecs.open('E:\\\\1Now\\\\mae\\\\beautifulExcluraw.txt', 'r', 'utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "lines = sorted(set(lines))\n",
    "print 'unique words:', len(lines)\n",
    "#lines = sorted(lines)\n",
    "\n",
    "fd = codecs.open('E:\\\\1Now\\\\mae\\\\beautifulExclu.txt', 'w', 'utf-8')\n",
    "for line in lines:\n",
    "    fd.write(line + '\\n')\n",
    "fd.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- mwBen statistics ---\n",
      "total head words:  39195\n",
      "total inflections:  64355\n",
      "total subhws:  9544\n",
      "total subhws whose 1st letters are different from hws:  334\n",
      "hw without any derivatives:  11182\n",
      "\n",
      "\n",
      "number of lines in mwBen before adding 84en_fr.txt:  85081\n",
      "number of unique lines in mwBen before adding 84en_fr.txt:  53972\n",
      "number of lines in 84en_fr.txt:  84\n",
      "number of unique lines in 84en_fr.txt:  83\n",
      "---common words of list-1 and list-2---: 15\n",
      "---words only in list-1---: 68\n",
      "---words only in list-2---: 53957\n",
      "number of lines in mwBen after adding 84en_fr.txt:  54040\n"
     ]
    }
   ],
   "source": [
    "# output mwlBenchmark.txt (mwBen) (no duplicate lines)\n",
    "\n",
    "# format: derivatives of hw (inflections, or subhws);hw\n",
    "# sort by 'derivatives;hw' and make every line unique\n",
    "# 2015-11-28, 2015-11-30, 2015-12-01\n",
    "\n",
    "# blazes;blaze\n",
    "# blazing;blaze\n",
    "# blazingly;blazing\n",
    "\n",
    "# I can know blaze, blazing are hws, the two lines below are not allowd because hws have derivates:\n",
    "# blaze;blaze\n",
    "# blazing;blazing\n",
    "# the two lines above are redundant.\n",
    "\n",
    "# manually delete: asas\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# codecs if for read and write files with unicode, encoding 'utf-8'\n",
    "import codecs\n",
    "\n",
    "#connect to MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.dpdb  # databas: dpdb\n",
    "geweiDict = db.geweiDict201511 # collection (table): geweiDict201511\n",
    "#geweiDict = db.geweiDictTest\n",
    "\n",
    "fdict = codecs.open('E:\\\\1Now\\\\taglist\\\\mwlBenchmarkraw.txt', 'w', 'utf-8')\n",
    "    \n",
    "results = geweiDict.find({}, {\"_id\":0, \"hw\":1, \"infl\":1, \"subhw\":1}) #.limit(1000)\n",
    "# sort method will use much more memory\n",
    "#results = geweiDict.find({}, {\"_id\":0, \"hw\":1, \"prn\":1, \"alles\":1, \"idpvs\":1, \"sublist\":1}).sort(\"hw\", 1).limit(1000)\n",
    "\n",
    "\n",
    "hwcnt = 0 # number of head words (hws)\n",
    "# hwcapcnt = 0 # number of hws capitalized\n",
    "inflcnt = 0 # number of inflections\n",
    "subhwcnt = 0 # number of subhws\n",
    "subhw1cnt = 0 # number of subhws whose first letter is not same as hw\n",
    "onlyhwcnt = 0\n",
    "\n",
    "# subhw1hw = []\n",
    "\n",
    "for result in results:\n",
    "    inflflag = 0\n",
    "    subhwflag = 0\n",
    "    \n",
    "    word = result.get(\"hw\")    \n",
    "    if word:\n",
    "        #print word\n",
    "#         fdict.write(word + ';' + word + '\\n')\n",
    "        hwcnt += 1      \n",
    "    else:\n",
    "        print \"no head word\"\n",
    "        continue\n",
    "\n",
    "    # get items of result\n",
    "    infls = result.get(\"infl\")\n",
    "    subhws = result.get(\"subhw\")     \n",
    "    \n",
    "    if infls:\n",
    "        for infl in infls:\n",
    "            if infl == word:\n",
    "#                 print 'infl == word', infl\n",
    "                continue\n",
    "            fdict.write(infl + ';' + word + '\\n')\n",
    "            inflcnt += 1   \n",
    "            inflflag = 1\n",
    "            \n",
    "    if subhws:\n",
    "        for subhw in subhws:\n",
    "            #if subhw[0] != word[0]: continue       \n",
    "            if subhw == word:\n",
    "#                 print 'subhw == word', infl\n",
    "                continue           \n",
    "            if subhw[0] != word[0]:\n",
    "#                 print 'subhw is not like word: ', subhw, '...', word\n",
    "                subhw1cnt += 1\n",
    "#                 subhw1hw.append(subhw+';'+word)\n",
    "            fdict.write(subhw + ';' + word + '\\n')\n",
    "            subhwcnt += 1      \n",
    "            subhwflag = 1\n",
    "            \n",
    "    # hw;hw (only when hw has no inflections and no subhws)\n",
    "    if inflflag == 0 and subhwflag == 0:\n",
    "        fdict.write(word + ';' + word + '\\n')\n",
    "        onlyhwcnt += 1\n",
    "    \n",
    "fdict.close()\n",
    "\n",
    "print '--- mwBen statistics ---'\n",
    "print \"total head words: \", hwcnt\n",
    "# print 'total hws capitalized: ', hwcapcnt\n",
    "print \"total inflections: \", inflcnt\n",
    "print 'total subhws: ', subhwcnt\n",
    "print 'total subhws whose 1st letters are different from hws: ', subhw1cnt\n",
    "print 'hw without any derivatives: ', onlyhwcnt\n",
    "\n",
    "\n",
    "with codecs.open('E:\\\\1Now\\\\taglist\\\\mwlBenchmarkraw.txt', 'r', 'utf-8') as f:\n",
    "    mwls = f.read().splitlines()\n",
    "print '\\n\\nnumber of lines in mwBen before adding 84en_fr.txt: ', len(mwls)\n",
    "mwls = sorted(set(mwls))\n",
    "print 'number of unique lines in mwBen before adding 84en_fr.txt: ', len(mwls)\n",
    "\n",
    "# read en_fr.txt\n",
    "with codecs.open('E:\\\\1Now\\\\taglist\\\\84en_fr.txt', 'r', 'utf-8') as f:\n",
    "    enfrs = f.read().splitlines()\n",
    "print 'number of lines in 84en_fr.txt: ', len(enfrs)\n",
    "enfrs = sorted(set(enfrs))\n",
    "print 'number of unique lines in 84en_fr.txt: ', len(set(enfrs))\n",
    "\n",
    "compareWordlist(l1=enfrs, l2=mwls)\n",
    "\n",
    "# sort and make unique\n",
    "lines = sorted(set(mwls + enfrs))\n",
    "print 'number of lines in mwBen after adding 84en_fr.txt: ', len(lines)\n",
    "\n",
    "fd = codecs.open('E:\\\\1Now\\\\taglist\\\\mwlBenchmark.txt', 'w', 'utf-8')\n",
    "for line in lines:\n",
    "    fd.write(line + '\\n')\n",
    "fd.close()\n",
    "\n",
    "\n",
    "# subhw1hw = sorted(set(subhw1hw))\n",
    "# fd = codecs.open('E:\\\\1Now\\\\taglist\\\\subhw1hw.txt', 'w', 'utf-8')\n",
    "# for x in subhw1hw:\n",
    "#     fd.write(x + '\\n')\n",
    "# fd.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- explore mwBen ---\n",
      "number of derivative column:  53972\n",
      "number of hw column:  53972\n",
      "unique number of hw column:  34398\n",
      "number of derivatives (inflections and subhws) whose 1st letter different from hws:  337\n",
      "number of capitalized hws:  1487\n"
     ]
    }
   ],
   "source": [
    "# explore mwBen\n",
    "\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "with codecs.open('E:\\\\1Now\\\\taglist\\\\mwlBenchmark.txt', 'r', 'utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "deriv = [] # derivative\n",
    "benhw = [] # hw\n",
    "deriv1hw = [] # derivative (inflection, subhw) first letter different from its hw\n",
    "hwcap = [] # hw whose first letter is capitalized\n",
    "for line in lines:\n",
    "    x1 = line.split(';')[0]\n",
    "    x2 = line.split(';')[1]\n",
    "    deriv.append(x1)\n",
    "    benhw.append(x2)\n",
    "    if x1[0] != x2[0]: deriv1hw.append(line)\n",
    "    if x2[0] in string.uppercase[0:26]: hwcap.append(x2)\n",
    "derivS = sorted(deriv)\n",
    "benhwSU = sorted(set(benhw)) # Sorted, Unique\n",
    "hwcap = sorted(set(hwcap))\n",
    "\n",
    "print '--- explore mwBen ---'\n",
    "print 'number of derivative column: ', len(deriv)\n",
    "print 'number of hw column: ', len(benhw)\n",
    "print 'unique number of hw column: ', len(benhwSU)\n",
    "print 'number of derivatives (inflections and subhws) whose 1st letter different from hws: ', len(deriv1hw)\n",
    "print 'number of capitalized hws: ', len(hwcap)\n",
    "\n",
    "fout = codecs.open('E:\\\\1Now\\\\taglist\\\\mwlStat.txt', 'w', 'utf-8')\n",
    "fout.write('--- statistics ---\\n')\n",
    "fout.write('number of lines in mwBen: ' + str(len(deriv)) + '\\n')\n",
    "fout.write('unique number of hw: ' + str(len(benhwSU)) + '\\n')\n",
    "fout.write('number of derivatives (inflections and subhws) whose 1st letter different from hws: ' + str(len(deriv1hw)) + '\\n')\n",
    "fout.write('number of capitalized hws: ' + str(len(hwcap)) + '\\n')\n",
    "\n",
    "fout.write('Contents\\n--------\\n')\n",
    "fout.write(\"Q: Are there duplicate derivatives who aren't hws in mwBen?\\n\")\n",
    "fout.write(\"Q: List derivatives (inflections and subhws) whose first letter is different from the first letter of their hws\\n\")\n",
    "fout.write(\"Capitalized hws\\n\\n\")\n",
    "\n",
    "# Q: Are there duplicate derivatives who aren't hws in mwBen?\n",
    "dupderiv = [x for x in derivS if x not in benhwSU and derivS.count(x)>1]\n",
    "fout.write(\"--- Duplicate derivatives not being hws: ---\\n\")\n",
    "for x in dupderiv:\n",
    "    fout.write(x + '\\n')\n",
    "\n",
    "# Q: List subhws whose first letter is different from the first letter of their hws\n",
    "fout.write(\"\\n--- derivatives (inflections and subhws) whose 1st letter different from hws: ---\\n\")\n",
    "fout.write(\"derivatives (inflections and subhws);hw\\n\\n\")\n",
    "for x in subhw1hw:\n",
    "    fout.write(x + '\\n')\n",
    "\n",
    "fout.write(\"\\n--- Capitalized hws: ---\\n\")\n",
    "for x in hwcap:\n",
    "    fout.write(x + '\\n')\n",
    "           \n",
    "fout.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n",
      "I'm 1\n",
      "I've 1\n",
      "Sunday 3\n",
      "Sunday 3\n",
      "Sunday 3\n",
      "absolutely 4\n",
      "absolutely 4\n",
      "absolutely 4\n",
      "absolutely 4\n",
      "adaptation 3\n",
      "adaptation 3\n",
      "adaptation 3\n",
      "all-nighter 1\n",
      "alongside 5\n",
      "alongside 5\n",
      "alongside 5\n",
      "alongside 5\n",
      "alongside 5\n",
      "animation 3\n",
      "animation 3\n",
      "animation 3\n",
      "aspect 4\n",
      "aspect 4\n",
      "aspect 4\n",
      "aspect 4\n",
      "assuage 1\n",
      "blissful 2\n",
      "blissful 2\n",
      "bloodstream 1\n",
      "caricature 4\n",
      "caricature 4\n",
      "caricature 4\n",
      "caricature 4\n",
      "cloudy 3\n",
      "cloudy 3\n",
      "cloudy 3\n",
      "consciousness 4\n",
      "consciousness 4\n",
      "consciousness 4\n",
      "consciousness 4\n",
      "deck 12\n",
      "deck 12\n",
      "deck 12\n",
      "deck 12\n",
      "deck 12\n",
      "deck 12\n",
      "deck 12\n",
      "deck 12\n",
      "deck 12\n",
      "deck 12\n",
      "deck 12\n",
      "deck 12\n",
      "decode 3\n",
      "decode 3\n",
      "decode 3\n",
      "definitely 1\n",
      "definitive 3\n",
      "definitive 3\n",
      "definitive 3\n",
      "dialogue 3\n",
      "dialogue 3\n",
      "dialogue 3\n",
      "doodle 2\n",
      "doodle 2\n",
      "ed 1\n",
      "editorial 3\n",
      "editorial 3\n",
      "editorial 3\n",
      "eight 5\n",
      "eight 5\n",
      "eight 5\n",
      "eight 5\n",
      "eight 5\n",
      "embrace 8\n",
      "embrace 8\n",
      "embrace 8\n",
      "embrace 8\n",
      "embrace 8\n",
      "embrace 8\n",
      "embrace 8\n",
      "embrace 8\n",
      "eyewitness 1\n",
      "fairly 4\n",
      "fairly 4\n",
      "fairly 4\n",
      "fairly 4\n",
      "fifth 6\n",
      "fifth 6\n",
      "fifth 6\n",
      "fifth 6\n",
      "fifth 6\n",
      "fifth 6\n",
      "filmmaker 1\n",
      "five 7\n",
      "five 7\n",
      "five 7\n",
      "five 7\n",
      "five 7\n",
      "five 7\n",
      "five 7\n",
      "fluent 4\n",
      "fluent 4\n",
      "fluent 4\n",
      "fluent 4\n",
      "fluke 1\n",
      "folk 7\n",
      "folk 7\n",
      "folk 7\n",
      "folk 7\n",
      "folk 7\n",
      "folk 7\n",
      "folk 7\n",
      "globe 4\n",
      "globe 4\n",
      "globe 4\n",
      "globe 4\n",
      "goofy 3\n",
      "goofy 3\n",
      "goofy 3\n",
      "he's 1\n",
      "headache 3\n",
      "headache 3\n",
      "headache 3\n",
      "headquarters 1\n",
      "intensive 5\n",
      "intensive 5\n",
      "intensive 5\n",
      "intensive 5\n",
      "intensive 5\n",
      "interact 3\n",
      "interact 3\n",
      "interact 3\n",
      "interconnect 2\n",
      "interconnect 2\n",
      "intuition 2\n",
      "intuition 2\n",
      "it's 1\n",
      "joe 3\n",
      "joe 3\n",
      "joe 3\n",
      "john 3\n",
      "john 3\n",
      "john 3\n",
      "laid-back 1\n",
      "largely 1\n",
      "laughter 1\n",
      "lee 1\n",
      "let's 1\n",
      "long-term 1\n",
      "luckily 1\n",
      "manageable 2\n",
      "manageable 2\n",
      "melancholy 2\n",
      "melancholy 2\n",
      "metaphor 4\n",
      "metaphor 4\n",
      "metaphor 4\n",
      "metaphor 4\n",
      "mom 1\n",
      "motivate 4\n",
      "motivate 4\n",
      "motivate 4\n",
      "motivate 4\n",
      "online 3\n",
      "online 3\n",
      "online 3\n",
      "opinionated 1\n",
      "outwardly 1\n",
      "producer 2\n",
      "producer 2\n",
      "purity 3\n",
      "purity 3\n",
      "purity 3\n",
      "quickly 1\n",
      "receipt 3\n",
      "receipt 3\n",
      "receipt 3\n",
      "redundancy 5\n",
      "redundancy 5\n",
      "redundancy 5\n",
      "redundancy 5\n",
      "redundancy 5\n",
      "regulate 3\n",
      "regulate 3\n",
      "regulate 3\n",
      "relatively 2\n",
      "relatively 2\n",
      "revelation 4\n",
      "revelation 4\n",
      "revelation 4\n",
      "revelation 4\n",
      "ripped 2\n",
      "ripped 2\n",
      "roil 2\n",
      "roil 2\n",
      "s 3\n",
      "s 3\n",
      "s 3\n",
      "sacred 5\n",
      "sacred 5\n",
      "sacred 5\n",
      "sacred 5\n",
      "sacred 5\n",
      "scary 3\n",
      "scary 3\n",
      "scary 3\n",
      "script 7\n",
      "script 7\n",
      "script 7\n",
      "script 7\n",
      "script 7\n",
      "script 7\n",
      "script 7\n",
      "seven 6\n",
      "seven 6\n",
      "seven 6\n",
      "seven 6\n",
      "seven 6\n",
      "seven 6\n",
      "she's 1\n",
      "six 8\n",
      "six 8\n",
      "six 8\n",
      "six 8\n",
      "six 8\n",
      "six 8\n",
      "six 8\n",
      "six 8\n",
      "slowly 2\n",
      "slowly 2\n",
      "spark 10\n",
      "spark 10\n",
      "spark 10\n",
      "spark 10\n",
      "spark 10\n",
      "spark 10\n",
      "spark 10\n",
      "spark 10\n",
      "spark 10\n",
      "spark 10\n",
      "spiel 1\n",
      "spy 6\n",
      "spy 6\n",
      "spy 6\n",
      "spy 6\n",
      "spy 6\n",
      "spy 6\n",
      "statistic 4\n",
      "statistic 4\n",
      "statistic 4\n",
      "statistic 4\n",
      "studio 7\n",
      "studio 7\n",
      "studio 7\n",
      "studio 7\n",
      "studio 7\n",
      "studio 7\n",
      "studio 7\n",
      "suddenly 1\n",
      "sunny 2\n",
      "sunny 2\n",
      "surroundings 1\n",
      "susceptible 2\n",
      "susceptible 2\n",
      "tame 10\n",
      "tame 10\n",
      "tame 10\n",
      "tame 10\n",
      "tame 10\n",
      "tame 10\n",
      "tame 10\n",
      "tame 10\n",
      "tame 10\n",
      "tame 10\n",
      "testimony 2\n",
      "testimony 2\n",
      "there's 1\n",
      "they're 1\n",
      "they've 1\n",
      "third 11\n",
      "third 11\n",
      "third 11\n",
      "third 11\n",
      "third 11\n",
      "third 11\n",
      "third 11\n",
      "third 11\n",
      "third 11\n",
      "third 11\n",
      "third 11\n",
      "three 8\n",
      "three 8\n",
      "three 8\n",
      "three 8\n",
      "three 8\n",
      "three 8\n",
      "three 8\n",
      "three 8\n",
      "tinge 3\n",
      "tinge 3\n",
      "tinge 3\n",
      "totally 1\n",
      "tricky 3\n",
      "tricky 3\n",
      "tricky 3\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "two 20\n",
      "ultimate 5\n",
      "ultimate 5\n",
      "ultimate 5\n",
      "ultimate 5\n",
      "ultimate 5\n",
      "uncertainty 2\n",
      "uncertainty 2\n",
      "unchanging 1\n",
      "unreliable 4\n",
      "unreliable 4\n",
      "unreliable 4\n",
      "unreliable 4\n",
      "upcoming 1\n",
      "usually 1\n",
      "visual 3\n",
      "visual 3\n",
      "visual 3\n",
      "visualize 2\n",
      "visualize 2\n",
      "we've 1\n",
      "weird 4\n",
      "weird 4\n",
      "weird 4\n",
      "weird 4\n",
      "whammy 1\n",
      "whereby 1\n",
      "who's 2\n",
      "who's 2\n",
      "wrestling 1\n",
      "yeah 3\n",
      "yeah 3\n",
      "yeah 3\n",
      "you're 1\n",
      "114\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
