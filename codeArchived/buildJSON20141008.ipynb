{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import json\n",
      "import re\n",
      "import os\n",
      "from os import listdir\n",
      "from os.path import isfile, join\n",
      "import shutil\n",
      "\n",
      "import pymongo\n",
      "from pymongo import MongoClient\n",
      "\n",
      "#connect to MongoDB\n",
      "client = MongoClient('localhost', 27017)\n",
      "db = client.dpdb\n",
      "dpdict = db.dpdict\n",
      "\n",
      "\n",
      "def a_has_id(tag):\n",
      "    return tag.has_attr('id')\n",
      "\n",
      "def a_has_href(tag):\n",
      "    return tag.has_attr('href')\n",
      "\n",
      "\n",
      "labels = ('US', 'chiefly US', 'Brit', 'chiefly Brit', \n",
      "          'slang', 'offensive', 'obscene', 'impolite', \n",
      "          'informal', 'formal', 'literary', 'old-fashioned', 'humorous', 'technical', 'disapproving', 'approving',\n",
      "          'medical', 'law', 'baseball', 'mathematics', 'sports', 'grammar',\n",
      "          'trademark', 'old-fashioned')\n",
      "\n",
      "#map IPA image symbols to my pronunciation system\n",
      "ipa_to_alpha = {\n",
      "    '22089': 'ZH',\n",
      "    '22212': 'ZH',\n",
      "    '22369': 'ZH',\n",
      "    '22077': 'z',\n",
      "    '22200': 'y',\n",
      "    '22047': 'w',\n",
      "    '22005': 'v',\n",
      "    '22218': 'u-Y',\n",
      "    '22167': 'u-B',\n",
      "    '22170': 'u-b',\n",
      "    '22203': 'u-b',\n",
      "    '22137': 'u-A',\n",
      "    '22140': 'u-a',\n",
      "    '22254': 'u-2',\n",
      "    '22152': 'u-1',\n",
      "    '22242': 'u-1',\n",
      "    '22263': 'u-1',\n",
      "    '22294': 'u-1',\n",
      "    '22312': 'u-1',\n",
      "    '22339': 'u-_',\n",
      "    '22185': 'u-^',\n",
      "    '22309': 'u-#',\n",
      "    '22321': 'u-#',\n",
      "    '22029': 'u',\n",
      "    '22074': 'u',\n",
      "    '22342': 'u',\n",
      "    '22101': 'TH',\n",
      "    '22206': 'TH',\n",
      "    '22366': 'TH',\n",
      "    '22038': 't',\n",
      "    '22044': 'SH',\n",
      "    '22209': 'SH',\n",
      "    '22363': 'SH',\n",
      "    '22014': 's',\n",
      "    '22050': 'r',\n",
      "    '22092': 'p',\n",
      "    '22281': 'p',\n",
      "    '22026': 'o',\n",
      "    '22068': 'NG',\n",
      "    '22360': 'NG',\n",
      "    '21981': 'n',\n",
      "    '22080': 'n',\n",
      "    '22357': 'n',\n",
      "    '22035': 'm',\n",
      "    '22227': 'm',\n",
      "    '22023': 'l',\n",
      "    '22053': 'l',\n",
      "    '22354': 'l',\n",
      "    '22008': 'k',\n",
      "    '22071': 'JH',\n",
      "    '22351': 'JH',\n",
      "    '22062': 'j',\n",
      "    '21966': 'i',\n",
      "    '22032': 'i',\n",
      "    '22333': 'i',\n",
      "    '22065': 'h',\n",
      "    '22110': 'h',\n",
      "    '22086': 'g',\n",
      "    '22083': 'f',\n",
      "    '21996': 'ER',\n",
      "    '22266': 'ER',\n",
      "    '22330': 'ER',\n",
      "    '21963': 'e',\n",
      "    '22059': 'e',\n",
      "    '22269': 'e',\n",
      "    '22345': 'e',\n",
      "    '22131': 'DH',\n",
      "    '22116': 'dd',\n",
      "    '21999': 'd',\n",
      "    '22104': 'CH',\n",
      "    '22348': 'CH',\n",
      "    '22119': 'b-a',\n",
      "    '22128': 'b-a',\n",
      "    '22011': 'b',\n",
      "    '21975': 'AH',\n",
      "    '22056': 'AH',\n",
      "    '22272': 'AH',\n",
      "    '22324': 'AH',\n",
      "    '22327': 'AH',\n",
      "    '21984': 'AE',\n",
      "    '21990': 'a',\n",
      "    '22020': 'a',\n",
      "    '22257': 'a',\n",
      "    '22336': 'a',\n",
      "    '22041': '~',\n",
      "    '22002': '`',\n",
      "    '21993': '_',\n",
      "    '22260': '_',\n",
      "    '21960': '^',\n",
      "    '22149': ';',\n",
      "    '21957': '/',\n",
      "    '21978': ',',\n",
      "    '21969': '#',\n",
      "    '21972': '#',\n",
      "    '21987': '#',\n",
      "    '22017': '#',\n",
      "    '22095': '#',\n",
      "    '22098': '#',\n",
      "    '22107': '#',\n",
      "    '22113': '#',\n",
      "    '22122': '#',\n",
      "    '22125': '#',\n",
      "    '22134': '#',\n",
      "    '22143': '#',\n",
      "    '22146': '#',\n",
      "    '22155': '#',\n",
      "    '22158': '#',\n",
      "    '22161': '#',\n",
      "    '22164': '#',\n",
      "    '22173': '#',\n",
      "    '22176': '#',\n",
      "    '22179': '#',\n",
      "    '22182': '#',\n",
      "    '22188': '#',\n",
      "    '22191': '#',\n",
      "    '22194': '#',\n",
      "    '22197': '#',\n",
      "    '22215': '#',\n",
      "    '22221': '#',\n",
      "    '22224': '#',\n",
      "    '22230': '#',\n",
      "    '22233': '#',\n",
      "    '22236': '#',\n",
      "    '22239': '#',\n",
      "    '22245': '#',\n",
      "    '22248': '#',\n",
      "    '22251': '#',\n",
      "    '22275': '#',\n",
      "    '22278': '#',\n",
      "    '22284': '#',\n",
      "    '22287': '#',\n",
      "    '22291': '#',\n",
      "    '22297': '#',\n",
      "    '22300': '#',\n",
      "    '22303': '#',\n",
      "    '22306': '#',\n",
      "    '22315': '#',\n",
      "    '22318': '#',\n",
      "    '22372': '#'\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "mypath = os.getcwd() + \"\\\\rawfile\"\n",
      "print mypath\n",
      "onlyfiles = [ f for f in os.listdir(mypath) if isfile(join(mypath,f)) ]\n",
      "\n",
      "cnt = 0  #total words\n",
      "cnt_f = 0 #number of French words\n",
      "cnt_d = 0 #number of words beginning with a dash\n",
      "cnt_sq = 0 #number of words beginning with a single quote\n",
      "\n",
      "kk = 0\n",
      "for fname in onlyfiles:\n",
      "    jsonBody = {}\n",
      "    \n",
      "    fj = open('json\\\\' + fname + '.json', 'w')\n",
      "    #print fj\n",
      "    \n",
      "    with open('rawfile\\\\' + fname, 'r') as fentry:\n",
      "        raw_doc = fentry.read()\n",
      "    #fentry = open('declare.axb8472', 'r')\n",
      "    #hw_doc = fentry.read()\n",
      "        \n",
      "    \n",
      "    #get IPA symbols\n",
      "    patt = '<img hspace=\"0\" align=\"middle\" losrc=\"Images\\/image21955\\.gif\" src=\"Images\\/image21956\\.gif\" hisrc=\"Images\\/image21957\\.gif\"\\/>'\n",
      "    pattern = patt + '(.*?)' + patt\n",
      "    #print pattern\n",
      "    ipa = re.compile(pattern, re.DOTALL)\n",
      "\n",
      "    flag_brit = 0 #suppose there is no British pronunciation\n",
      "    my_pronun = []\n",
      "    for sounds in ipa.findall(raw_doc):\n",
      "        i = 0\n",
      "        for sound in sounds.split('<i>Brit</i>'):\n",
      "            i += 1\n",
      "            ipa_soup = BeautifulSoup(sound)\n",
      "            if i == 1: cmu = ''\n",
      "            elif i == 2: #'Brit'\n",
      "                flag_brit = 1 #there is British pronunciation\n",
      "                break #skip British pronunciation\n",
      "            for symbol in ipa_soup.find_all('img'):\n",
      "                key = symbol['hisrc'][12:-4]\n",
      "                if key in ipa_to_alpha: cmu += ipa_to_alpha[key]\n",
      "                else: cmu += key\n",
      "            #print cmu\n",
      "            #print \"---\\n\"   \n",
      "            my_pronun.append(cmu)\n",
      "    #print my_pronun\n",
      "    if my_pronun: jsonBody['prn'] = my_pronun\n",
      "    if flag_brit == 1: jsonBody['prn_brit'] = 'y'\n",
      "    \n",
      "    \n",
      "    \n",
      "    #after getting IPA, remove all image tags    \n",
      "    hw_doc = re.sub(r\"(<img[^>]*\\/>)\", \"\", raw_doc)\n",
      "    #remove Unicode 'middle dot' u+00B7\n",
      "    hw_doc = re.sub(u\"\\u00B7\", \"\", hw_doc)\n",
      "    #print hw_doc\n",
      "    #continue\n",
      "    fentry.close()\n",
      "\n",
      "    soup = BeautifulSoup(hw_doc)\n",
      "\n",
      "    \n",
      "    #get headword\n",
      "    headword = soup.find('idx:orth')\n",
      "    #if headword[0] == \"'\": headword = \"\\\\\" + headword\n",
      "    if headword: \n",
      "        jsonBody['hw'] = headword['value']\n",
      "        kk += 1\n",
      "        print kk, '---', jsonBody\n",
      "    else:\n",
      "        print fname, \": no headword\"\n",
      "        break\n",
      "    \n",
      "    #check if it is in 3,000 basic English words (3,000 core vocabulary words)\n",
      "    basic_word = soup.find('u')\n",
      "    if basic_word: jsonBody['bw'] = 1    \n",
      "    #print jsonBody\n",
      "    \n",
      "\n",
      "    #French words\n",
      "    if fname[0] == '.':\n",
      "        cnt_f += 1\n",
      "        jsonBody[\"fr\"] = \"y\" #is it a Frenche word? yes\n",
      "    elif fname[0] == '-':\n",
      "        cnt_d += 1\n",
      "        jsonBody[\"dash\"] = \"y\" # begin with a dash\n",
      "    elif fname[0] == \"'\":\n",
      "        cnt_sq += 1\n",
      "        jsonBody[\"sq\"] = \"y\" #begin with a single quote '\n",
      "    \n",
      "    \n",
      "    #get inflection\n",
      "    infl = []\n",
      "    for it in soup.find_all('idx:iform'):\n",
      "        infl.append(it['value'])\n",
      "        #print infl\n",
      "    if infl: jsonBody['infl'] = infl\n",
      "\n",
      "    #get id\n",
      "    aid = soup.find(a_has_id)\n",
      "    #print type(aid)\n",
      "    if aid: jsonBody['id'] = aid['id']\n",
      "\n",
      "    #get part of speech\n",
      "    #<i><font color=\"#999999\">noun</font></i>\n",
      "    pos = soup.find(color=\"#999999\")\n",
      "    if pos: jsonBody['pos'] = pos.get_text()\n",
      "\n",
      "\n",
      "    #get labels\n",
      "    label = []\n",
      "    label = soup.find_all(text=labels)\n",
      "    #some labels are not captured in tags, so use regex to get them\n",
      "    if soup.find(text=re.compile(\"figuratively\")): label.append(\"figuratively\")\n",
      "    if soup.find(text=re.compile(\"informal\")): label.append(\"informal\")\n",
      "    #no 'see color picture' in mobi file\n",
      "    #if soup.find_all(text=re.compile(\"see color picture\")): label.append(\"color picture\")\n",
      "    if label: jsonBody['label'] = label\n",
      "\n",
      "    #get common phrases\n",
      "    cplist = []\n",
      "    for item in soup.find_all('span'):\n",
      "        if item.parent.name == 'font': cplist.append(item.get_text())\n",
      "    if cplist: jsonBody['cp'] = cplist\n",
      "\n",
      "    #get all example sentences\n",
      "    examples = []\n",
      "    for it in soup.find_all(color=\"#002984\"):\n",
      "        examples.append(it.get_text())\n",
      "    if examples: jsonBody['alles'] = examples\n",
      "\n",
      "    #get all idioms & phrasal verbs, including common phrases\n",
      "    idpvs = []\n",
      "    for item in soup.find_all('span'):\n",
      "        #print item.parent.name, \"---\", item.get_text()\n",
      "        if item.parent.name != 'idx:entry': idpvs.append(item.get_text())\n",
      "    if idpvs: jsonBody['idpvs'] = idpvs\n",
      "                \n",
      "                \n",
      "    #get cross references\n",
      "    cr = {}\n",
      "    for link in soup.find_all(a_has_href):\n",
      "        if link: cr[link.get('href').replace('#','')] = link.get_text().strip()\n",
      "    if cr: jsonBody['cr'] = cr\n",
      "\n",
      "        \n",
      "    #construct definitions & examples\n",
      "    sublist = []\n",
      "    i = 1 #cross reference counter\n",
      "    j = 1 #idioms & phrasal verbs counter\n",
      "\n",
      "    for it in soup.find_all('blockquote'):\n",
      "        #print it.parent.parent.name, \">>\", it.parent.name, \"---\", it, \"\\n\"\n",
      "        if(it.parent.name=='idx:entry'):\n",
      "            examp = []\n",
      "            defi = it.get_text()\n",
      "            if defi==\" \": continue\n",
      "            for e in it.find_all(color=\"#002984\"):\n",
      "                examp.append(e.get_text())\n",
      "                defi = defi.replace(e.get_text(),\"\")\n",
      "            #print defi\n",
      "            #print examp\n",
      "\n",
      "            subentry = {}\n",
      "            subentry['def'] = defi\n",
      "            subentry['es'] = examp\n",
      "            #print subentry\n",
      "            #print sublist                \n",
      "\n",
      "            if subentry: sublist.append(subentry)\n",
      "        #idioms & phrasal verbs\n",
      "        if(it.parent.name=='div' and it.parent.parent.name!='blockquote'):\n",
      "            subentry = {}\n",
      "            subentry['idpv'+str(j)] = it.get_text()\n",
      "            j += 1\n",
      " \n",
      "            #print subentry\n",
      "            if subentry: sublist.append(subentry)\n",
      "\n",
      "            \n",
      "    if sublist: jsonBody['sublist'] = sublist\n",
      "    if jsonBody:\n",
      "        #print jsonBody\n",
      "        cnt += 1\n",
      "        json.dump(jsonBody, fj)\n",
      "        #doc_id = dpdict.insert(jsonBody)\n",
      "        #print doc_id \n",
      "        #shutil.move(\"rawfile\\\\\"+fname, \"processed\\\\\"+fname)\n",
      "    fj.close()\n",
      "\n",
      "print 'total words: ', cnt, ', French words: ', cnt_f, \", begin with a dash:\", cnt_d, \", begin with a ': \", cnt_sq\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C:\\Users\\GW\\iWork\\ProjectDict\\rawfile\n",
        "total words:  0 , French words:  0 , begin with a dash: 0 , begin with a ':  0\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(soup.img.decompose())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "None\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dpdict.find({\"label\":\"mathematics\"}).count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 193,
       "text": [
        "64"
       ]
      }
     ],
     "prompt_number": 193
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dpdict.find({\"label\":\"slang\"}).count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 194,
       "text": [
        "29"
       ]
      }
     ],
     "prompt_number": 194
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dpdict.find({\"label\":\"offensive\"}).count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 195,
       "text": [
        "33"
       ]
      }
     ],
     "prompt_number": 195
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dpdict.find({\"label\":\"chiefly US\"}).count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 196,
       "text": [
        "807"
       ]
      }
     ],
     "prompt_number": 196
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dpdict.find({\"label\":\"Brit\"}).count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 197,
       "text": [
        "2615"
       ]
      }
     ],
     "prompt_number": 197
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db.dpdict.find({\"label\":\"US\"}).count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 209,
       "text": [
        "2027"
       ]
      }
     ],
     "prompt_number": 209
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db.dpdict.find({cp: {$exists: true}}).count() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-208-11f5d97b9c3b>, line 1)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-208-11f5d97b9c3b>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    db.dpdict.find({cp: {$exists: true}}).count()\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 208
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dpdict.find_one({'hw': u'doppelg\\xe4nger'})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 284,
       "text": [
        "{u'_id': ObjectId('537e93cd8707ee3128063ada'),\n",
        " u'alles': [u'I saw your doppelg\\xe4nger [=(more commonly) double] yesterday.',\n",
        "  u'In the story, the character is haunted by a doppelg\\xe4nger.'],\n",
        " u'fr': u'y',\n",
        " u'hw': u'doppelg\\xe4nger',\n",
        " u'idpvs': [u'dop\\xb7pel\\xb7gang\\xb7er'],\n",
        " u'infl': {u'1np1': u'doppelg\\xe4ngers', u'1ns1': u'doppelg\\xe4nger'},\n",
        " u'label': [u'literary'],\n",
        " u'pos': u'or',\n",
        " u'sublist': [{u'def': u'1 : someone who looks like someone else  ',\n",
        "   u'es': [u'I saw your doppelg\\xe4nger [=(more commonly) double] yesterday.']},\n",
        "  {u'def': u'2 literary : a ghost that looks like a living person ',\n",
        "   u'es': [u'In the story, the character is haunted by a doppelg\\xe4nger.']}]}"
       ]
      }
     ],
     "prompt_number": 284
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dpdict.find({\"id\":\"filepos18005907\"})\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 233,
       "text": [
        "<pymongo.cursor.Cursor at 0x7993630>"
       ]
      }
     ],
     "prompt_number": 233
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "comment = re.compile(r'/\\*(.*?)\\*/', re.DOTALL)\n",
      "patt = '<img hspace=\"0\" align=\"middle\" losrc=\"Images\\/image21955\\.gif\" src=\"Images\\/image21956\\.gif\" hisrc=\"Images\\/image21957\\.gif\"\\/>'\n",
      "pattern = patt + '(.*?)' + patt\n",
      "print pattern\n",
      "ipa = re.compile(pattern, re.DOTALL)\n",
      "text1 = '/* this is a comment */'\n",
      "text2 = '''/* this is a\n",
      "        multiline comment */\n",
      "        '''\n",
      "text3 = '''<idx:entry scriptable=\"yes\"><idx:orth value=\"vast\"><idx:infl>  <idx:iform name=\"1.adj.pos.1\" value=\"vast\"/></idx:infl><idx:infl>  <idx:iform name=\"2.n.s.1\" value=\"vast\"/>  <idx:iform name=\"2.n.p.1\" value=\"vasts\"/></idx:infl></idx:orth><a id=\"filepos83514877\" /><span><font size=\"3\"><b><u>vast</u></span> <img hspace=\"0\" align=\"middle\" losrc=\"Images/image21955.gif\" src=\"Images/image21956.gif\" hisrc=\"Images/image21957.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image21958.gif\" src=\"Images/image21959.gif\" hisrc=\"Images/image21960.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image22003.gif\" src=\"Images/image22004.gif\" hisrc=\"Images/image22005.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image21982.gif\" src=\"Images/image21983.gif\" hisrc=\"Images/image21984.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image22012.gif\" src=\"Images/image22013.gif\" hisrc=\"Images/image22014.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image22036.gif\" src=\"Images/image22037.gif\" hisrc=\"Images/image22038.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image21976.gif\" src=\"Images/image21977.gif\" hisrc=\"Images/image21978.gif\"/> <i>Brit</i> <img hspace=\"0\" align=\"middle\" losrc=\"Images/image21958.gif\" src=\"Images/image21959.gif\" hisrc=\"Images/image21960.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image22003.gif\" src=\"Images/image22004.gif\" hisrc=\"Images/image22005.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image21988.gif\" src=\"Images/image21989.gif\" hisrc=\"Images/image21990.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image21991.gif\" src=\"Images/image21992.gif\" hisrc=\"Images/image21993.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image22012.gif\" src=\"Images/image22013.gif\" hisrc=\"Images/image22014.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image22036.gif\" src=\"Images/image22037.gif\" hisrc=\"Images/image22038.gif\"/><img hspace=\"0\" align=\"middle\" losrc=\"Images/image21955.gif\" src=\"Images/image21956.gif\" hisrc=\"Images/image21957.gif\"/> <i><font color=\"#999999\">adj</font></i>, <b>vast\u7a4dr</b>, <b>-est</b> [<i>more <img hspace=\"0\" align=\"middle\" losrc=\"Images/image22039.gif\" src=\"Images/image22040.gif\" hisrc=\"Images/image22041.gif\"/>; most <img hspace=\"0\" align=\"middle\" losrc=\"Images/image22039.gif\" src=\"Images/image22040.gif\" hisrc=\"Images/image22041.gif\"/></i>]<blockquote align=\"left\"> <b>:</b> very great in size, amount, or extent <blockquote align=\"left\"><font color=\"#002984\">She has a <i>vast</i> amount of knowledge on this subject.</font></blockquote> <blockquote align=\"left\"><font color=\"#002984\"><i>vast</i> quantities of information</font></blockquote> <blockquote align=\"left\"><font color=\"#002984\">The policy is supported by the <i>vast</i> majority of citizens.</font></blockquote> <blockquote align=\"left\"><font color=\"#002984\">a <i>vast</i> expanse of land</font></blockquote></blockquote> <img hspace=\"0\" vspace=\"0\" align=\"middle\" src=\"Images/image21970.gif\"/><div align=\"left\"><blockquote><span><font size=\"3\"><b>vast\u7a55y</span> <i><font color=\"#999999\">adv</font></i> <blockquote><font color=\"#002984\">His background is <i>vastly</i> different from mine.</font></blockquote> <blockquote><font color=\"#002984\">They <i>vastly</i> increased spending.</font></blockquote></blockquote> </div>\n",
      "\n",
      "<img hspace=\"0\" vspace=\"0\" align=\"middle\" src=\"Images/image21970.gif\"/><div align=\"left\"><blockquote><span><font size=\"3\"><b>vast\u7a58ess</span> \n",
      "\n",
      "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image21955.gif\" src=\"Images/image21956.gif\" hisrc=\"Images/image21957.gif\"/>\n",
      "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image21958.gif\" src=\"Images/image21959.gif\" hisrc=\"Images/image21960.gif\"/>\n",
      "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image22003.gif\" src=\"Images/image22004.gif\" hisrc=\"Images/image22005.gif\"/>\n",
      "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image21982.gif\" src=\"Images/image21983.gif\" hisrc=\"Images/image21984.gif\"/>\n",
      "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image22012.gif\" src=\"Images/image22013.gif\" hisrc=\"Images/image22014.gif\"/>\n",
      "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image22036.gif\" src=\"Images/image22037.gif\" hisrc=\"Images/image22038.gif\"/>\n",
      "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image21979.gif\" src=\"Images/image21980.gif\" hisrc=\"Images/image21981.gif\"/>\n",
      "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image21973.gif\" src=\"Images/image21974.gif\" hisrc=\"Images/image21975.gif\"/>\n",
      "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image22012.gif\" src=\"Images/image22013.gif\" hisrc=\"Images/image22014.gif\"/>\n",
      "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image21976.gif\" src=\"Images/image21977.gif\" hisrc=\"Images/image21978.gif\"/>\n",
      " <i>Brit</i> \n",
      " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image21958.gif\" src=\"Images/image21959.gif\" hisrc=\"Images/image21960.gif\"/>\n",
      " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image22003.gif\" src=\"Images/image22004.gif\" hisrc=\"Images/image22005.gif\"/>\n",
      " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image21988.gif\" src=\"Images/image21989.gif\" hisrc=\"Images/image21990.gif\"/>\n",
      " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image21991.gif\" src=\"Images/image21992.gif\" hisrc=\"Images/image21993.gif\"/>\n",
      " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image22012.gif\" src=\"Images/image22013.gif\" hisrc=\"Images/image22014.gif\"/>\n",
      " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image22036.gif\" src=\"Images/image22037.gif\" hisrc=\"Images/image22038.gif\"/>\n",
      " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image21979.gif\" src=\"Images/image21980.gif\" hisrc=\"Images/image21981.gif\"/>\n",
      " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image21973.gif\" src=\"Images/image21974.gif\" hisrc=\"Images/image21975.gif\"/>\n",
      " <img hspace=\"0\" align=\"middle\" losrc=\"Images/image22012.gif\" src=\"Images/image22013.gif\" hisrc=\"Images/image22014.gif\"/>\n",
      "<img hspace=\"0\" align=\"middle\" losrc=\"Images/image21955.gif\" src=\"Images/image21956.gif\" hisrc=\"Images/image21957.gif\"/> \n",
      "\n",
      "\n",
      "<i><font color=\"#999999\">noun</font></i> [<i>noncount</i>] <blockquote><font color=\"#002984\">the <i>vastness</i> of the desert/ocean</font></blockquote></blockquote></div></idx:entry><div><img hspace=\"0\" vspace=\"0\" align=\"middle\" src=\"Images/image21971.gif\"/></div><div><table width=\"100%\" bgcolor=\"#7593CD\"><tr><th widht=\"100%\" height=\"2px\"/></tr></table></div><div><img hspace=\"0\" vspace=\"0\" align=\"middle\" src=\"Images/image21972.gif\"/></div> \n",
      " \n",
      " '''\n",
      "#comment.findall(text1)\n",
      "#comment.findall(text2)\n",
      "for sounds in ipa.findall(text3):\n",
      "    for sound in sounds.split('<i>Brit</i>'):\n",
      "        print \"---\\n\"\n",
      "        soup = BeautifulSoup(sound)\n",
      "        for symbol in soup.find_all('img'):\n",
      "            print symbol['hisrc']\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<img hspace=\"0\" align=\"middle\" losrc=\"Images\\/image21955\\.gif\" src=\"Images\\/image21956\\.gif\" hisrc=\"Images\\/image21957\\.gif\"\\/>(.*?)<img hspace=\"0\" align=\"middle\" losrc=\"Images\\/image21955\\.gif\" src=\"Images\\/image21956\\.gif\" hisrc=\"Images\\/image21957\\.gif\"\\/>\n",
        "---\n",
        "\n",
        "Images/image21960.gif\n",
        "Images/image22005.gif\n",
        "Images/image21984.gif\n",
        "Images/image22014.gif\n",
        "Images/image22038.gif\n",
        "Images/image21978.gif\n",
        "---\n",
        "\n",
        "Images/image21960.gif\n",
        "Images/image22005.gif\n",
        "Images/image21990.gif\n",
        "Images/image21993.gif\n",
        "Images/image22014.gif\n",
        "Images/image22038.gif\n",
        "---\n",
        "\n",
        "Images/image21960.gif\n",
        "Images/image22005.gif\n",
        "Images/image21984.gif\n",
        "Images/image22014.gif\n",
        "Images/image22038.gif\n",
        "Images/image21981.gif\n",
        "Images/image21975.gif\n",
        "Images/image22014.gif\n",
        "Images/image21978.gif\n",
        "---\n",
        "\n",
        "Images/image21960.gif\n",
        "Images/image22005.gif\n",
        "Images/image21990.gif\n",
        "Images/image21993.gif\n",
        "Images/image22014.gif\n",
        "Images/image22038.gif\n",
        "Images/image21981.gif\n",
        "Images/image21975.gif\n",
        "Images/image22014.gif\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "My Phoneme Set (dpdict)\n",
      "\n",
      "IPA\tdpdict\tcmudict\tIPA\tdpdict\tcmudict\n",
      "\u00e6\tAE\tAE\tb\tb\tB\n",
      "\u0251:\ta_\tAA\td\td\tD\n",
      "\u025b\te\tEH\t\u02a4\tJH\tJH\n",
      "\u0259\tAH\tAH\t\u00f0\tDH\tDH\n",
      "i:\ti_\tIY\tf\tf\tF\n",
      "\u026a\ti\tIH\tg\tg\tG\n",
      "\u028a\tu\tUH\th\th\tH\n",
      "u:\tu_\tUW\tj\tj\tY\n",
      "\u028c\tAH\t\tk\tk\tK\n",
      "\u025a\tER\tER\tl\tl\tL\n",
      "e\u026a\tei\tEY\tl\u031f\tl\t\n",
      "a\u026a\tai\tAY\tm\tm\tM\n",
      "a\u028a\tau\tAW\tn\tn\tN\n",
      "o\u026a\toi\tOY\tn\u0329\tn\t\n",
      "o\u028a\tou\tOW\t\u014b\tNG\tNG\n",
      "\u0251\u025a\taER\t\tp\tp\tP\n",
      "e\u025a\teER\t\tr\tr\tR\n",
      "i\u025a\tiER\t\ts\ts\tS\n",
      "o\u025a\toER\t\t\u0283\tSH\tSH\n",
      "u\u025a\tuER\t\tt\tt\tT\n",
      "\t\t\tt\u0283\tCH\tCH\n",
      "\t\t\t\u03b8\tTH\tTH\n",
      "\t\t\tv\tv\tV\n",
      "\t\t\tw\tw\tW\n",
      "\t\t\tz\tz\tZ\n",
      "\t\t\t\u0292\tZH\tZH\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pymongo\n",
      "from pymongo import MongoClient\n",
      "\n",
      "#connect to MongoDB\n",
      "client = MongoClient('localhost', 27017)\n",
      "db = client.dpdb\n",
      "dpdict = db.dpdict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words = dpdict.find({\"hw\":\"incongruous\"})\n",
      "for word in words:\n",
      "    print word.get(\"alles\")\n",
      "    print word.get(\"sublist\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'His outburst seemed incongruous to those who know him well.', u'The style of the porch is incongruous with [=does not match] the style of the house overall.', u'The modern sculpture seems incongruous [=out of place] among all the antiques.']\n",
        "[{u'def': u' : strange because of not agreeing with what is usual or expected   ', u'es': [u'His outburst seemed incongruous to those who know him well.', u'The style of the porch is incongruous with [=does not match] the style of the house overall.', u'The modern sculpture seems incongruous [=out of place] among all the antiques.']}, {u'idpv1': u'in\\xb7con\\xb7gru\\xb7i\\xb7ty noun, pl -ties [count, noncount]'}, {u'idpv2': u'in\\xb7con\\xb7gru\\xb7ous\\xb7ly adv'}]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}